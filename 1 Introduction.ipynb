{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e90c10d-2753-4b92-95d7-0d5d25d86285",
   "metadata": {},
   "source": [
    "# Prepare the Datasets\n",
    "Before we start training a model and generating pseudo-words, we will need to prepare our datasets.\n",
    "Hunspell dictionaries are a good place to start to collect a sizable list of lemmas, it can also be used at a later stage to ensure that the pseudo-words generated are not inflected versions of real words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627fec0f-ddf4-4e01-a7f5-4927b37f45ed",
   "metadata": {},
   "source": [
    "## Download a Hunspell Dictionary\n",
    "Clicking on [this link](https://mozilla-l10n.github.io/firefox-dictionaries/complete.html) you will find a list of available and up-to-date dictionaries.\n",
    "Find the dictionary you want to train your model on. Once on the page of the dictionary you want to download, instead of clicking on \"add to Firefox\", right-click and select \"copy the link\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c42507ac-eeae-46dc-81b6-622824561b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/alan/miniconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: spylls in /Users/alan/miniconda3/lib/python3.12/site-packages (0.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alan/miniconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alan/miniconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alan/miniconda3/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alan/miniconda3/lib/python3.12/site-packages (from requests) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests spylls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "329093d3-ef45-4534-969b-e53de9946755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cy_GB dictionary decompressed\n",
      "Folder cleaned successfully.\n",
      "Check out your dictionary in dictionaries/cy_GB\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create a directory to store the dictionaries\n",
    "os.makedirs(\"dictionaries\", exist_ok=True)\n",
    "\n",
    "# Replace this link\n",
    "# for Breton (be): https://addons.mozilla.org/firefox/downloads/file/4270474/difazier_an_drouizig-0.17resigned1.xpi\n",
    "# for Welsh (cy_GB): https://addons.mozilla.org/firefox/downloads/file/4270302/geiriadur_cymraeg-1.8.3resigned1.xpi\n",
    "# for English (en-GB): https://addons.mozilla.org/firefox/downloads/file/4270302/geiriadur_cymraeg-1.8.3resigned1.xpi\n",
    "dictionary_url = \"https://addons.mozilla.org/firefox/downloads/file/4270302/geiriadur_cymraeg-1.8.3resigned1.xpi\"\n",
    "# Replace with the appropriate ISO-369 code\n",
    "locale = \"cy_GB\"\n",
    "\n",
    "if os.path.isdir(f\"dictionaries/{locale}\"):\n",
    "    shutil.rmtree(f\"dictionaries/{locale}\")\n",
    "\n",
    "# Download and extract dictionary\n",
    "response = requests.get(dictionary_url)\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    z.extractall(\"dictionaries\")\n",
    "    print(f\"{locale} dictionary decompressed\")\n",
    "\n",
    "# standardize name of files\n",
    "for file in os.listdir(f\"dictionaries/\"):\n",
    "    if file.endswith(\".dic\"):\n",
    "        os.rename(f\"dictionaries/dictionaries/{file}\", f\"dictionaries/{locale}.dic\")\n",
    "    elif file.endswith(\".aff\"):\n",
    "        os.rename(f\"dictionaries/dictionaries/{file}\", f\"dictionaries/{locale}.aff\")\n",
    "\n",
    "try:\n",
    "    files = os.listdir(\"dictionaries\")\n",
    "    shutil.rmtree(\"dictionaries/META-INF/\")\n",
    "    for file in files:\n",
    "        file_path = os.path.join(\"dictionaries\", file)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "    print(\"Folder cleaned successfully.\")\n",
    "except OSError:\n",
    "    print(\"Error occurred while deleting files.\")\n",
    "\n",
    "os.rename(\"dictionaries/dictionaries\", f\"dictionaries/{locale}\")\n",
    "print(\"Check out your dictionary in\", f\"dictionaries/{locale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa9ca23-c493-4145-a20a-a0b538f76e26",
   "metadata": {},
   "source": [
    "You should have a none-empty `dictionaries` showing up in file browser in the left. Next, we will extract a list of lemmas from these dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a10653-17fc-4a15-9e5e-c31a93d4a889",
   "metadata": {},
   "source": [
    "## Extract and Dump the List of lemmas\n",
    "For various reasons, we don't want to end up with all the entries from the dictionary. Some may be proper names or contain numbers (e.g. 4x4) that don't allow them to qualify as lemmas. Here, for English lemmas, we used the following regex to filter words only containing lowercase letter: `re.compile('[a-z]+')`.\n",
    "Depending on the language you want to treat, you may want to extend this regex, for example, Breton allows `c'h` as a common trigram, and the `'` character would need to be added, half of the Check alphabet would also be excluded from this regex (à, č, ź...) and I am not mentioning the letters from the Cyrillic or Greek alphabets... So think of adapting the regex in order not to miss common characters in your training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9f89f66-5367-44f4-a659-06d1bd32ef8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'ab', 'abaci', 'abacws', 'abad', 'abadaeth', 'abades', 'aballu', 'abatai', 'abatir']\n"
     ]
    }
   ],
   "source": [
    "from spylls.hunspell import Dictionary\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Function to extract lemmas from a .dic file\n",
    "def extract_lemmas(dic_path):\n",
    "    lemmas = []\n",
    "    lowercase = re.compile('[a-zŵŷ]+')\n",
    "    with open(dic_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Each line in the .dic file contains a word followed by its affix flags\n",
    "            # We only need the word part, which is before the '/' character\n",
    "            word = line.split('/')[0]\n",
    "            word2 = lowercase.findall(word)\n",
    "            if len(word2) and word == word2[0]:\n",
    "                lemmas.append(word)\n",
    "    return lemmas\n",
    "\n",
    "# Path to the dictionary files\n",
    "dic_path = f\"dictionaries/{locale}/{locale}.dic\"\n",
    "\n",
    "# Extract lemmas\n",
    "lemmas = extract_lemmas(dic_path)\n",
    "\n",
    "# Print the first 10 lemmas as a sample\n",
    "print(lemmas[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7470530",
   "metadata": {},
   "source": [
    "Now, if we are happy with the items being displayed, we can carry on and dump them in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "898f1c47-6309-4462-8e57-72f71c74c59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58953 items extracted and saved to dictionaries/cy_GB/lemmas.json\n"
     ]
    }
   ],
   "source": [
    "# Dump the lemmas to a json file\n",
    "import json\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = f\"dictionaries/{locale}/lemmas.json\"\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(lemmas, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"{len(lemmas)} items extracted and saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f7d241-092b-4287-98ac-4e2af8009b94",
   "metadata": {},
   "source": [
    "If you see the number of items you successfully extracted, you're ready for the next part.\n",
    "Please note that this is only one way to construct a list of words. Depending on the language, you may want to do web scrapping on an online dictionary. But every online dictionary has a different interface, and in that case you would have had to rewrite your own code. This is why we used a Hunspell dictionary, but it is definitely not a definitive nor universal way to do things."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
