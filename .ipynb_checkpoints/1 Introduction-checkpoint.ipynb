{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e90c10d-2753-4b92-95d7-0d5d25d86285",
   "metadata": {},
   "source": [
    "# Prepare the Datasets\n",
    "Before we start training a model and generating pseudo-words, we will need to prepare our datasets.\n",
    "Hunspell dictionaries are a good place to start to collect a sizable list of lemmas, it can also be used at a later stage to ensure that the pseudo-words generated are not inflected versions of real words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627fec0f-ddf4-4e01-a7f5-4927b37f45ed",
   "metadata": {},
   "source": [
    "## Download a Hunspell Dictionary\n",
    "Clicking on [this link](https://mozilla-l10n.github.io/firefox-dictionaries/complete.html) you will find a list of available and up-to-date dictionaries.\n",
    "Find the dictionary you want to train your model on. Once on the page of the dictionary you want to download, instead of clicking on \"add to Firefox\", right-click and select \"copy the link\". Then past the value to assign it to the variable `dictionary_url`.\n",
    "Think to also set the value of the variable `locale`, checkout the column \"Dictionary Locale\" of the table in the list of the dictionaries, as they might not be shaped the same way, for example, Welsh is \"cy_GB\" but British English is \"en-GB\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c42507ac-eeae-46dc-81b6-622824561b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/alan/miniconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: spylls in /Users/alan/miniconda3/lib/python3.12/site-packages (0.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alan/miniconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alan/miniconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alan/miniconda3/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alan/miniconda3/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests spylls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "329093d3-ef45-4534-969b-e53de9946755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "br dictionary decompressed\n",
      "Folder cleaned successfully.\n",
      "Check out your dictionary in locales/br\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create a directory to store the dictionaries\n",
    "os.makedirs(\"locales\", exist_ok=True)\n",
    "\n",
    "# Replace this link\n",
    "# for Breton (be): https://addons.mozilla.org/firefox/downloads/file/4270474/difazier_an_drouizig-0.17resigned1.xpi\n",
    "# for Welsh (cy_GB): https://addons.mozilla.org/firefox/downloads/file/4270302/geiriadur_cymraeg-1.8.3resigned1.xpi\n",
    "# for English (en-GB): https://addons.mozilla.org/firefox/downloads/file/4270302/geiriadur_cymraeg-1.8.3resigned1.xpi\n",
    "# for Dutch (nl): https://addons.mozilla.org/firefox/downloads/file/3776797/woordenboek_nederlands-4.20.19.xpi\n",
    "dictionary_url = \"https://addons.mozilla.org/firefox/downloads/file/4270474/difazier_an_drouizig-0.17resigned1.xpi\"\n",
    "\n",
    "# Replace with the appropriate ISO-369 code\n",
    "locale = \"br\"\n",
    "\n",
    "if os.path.isdir(f\"locales/{locale}\"):\n",
    "    shutil.rmtree(f\"locales/{locale}\")\n",
    "\n",
    "# Download and extract dictionary\n",
    "response = requests.get(dictionary_url)\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    z.extractall(\"locales\")\n",
    "    print(f\"{locale} dictionary decompressed\")\n",
    "\n",
    "# standardize name of files\n",
    "for file in os.listdir(f\"locales/\"):\n",
    "    if file.endswith(\".dic\"):\n",
    "        os.rename(f\"locales/dictionaries/{file}\", f\"locales/{locale}.dic\")\n",
    "    elif file.endswith(\".aff\"):\n",
    "        os.rename(f\"locales/dictionaries/{file}\", f\"locales/{locale}.aff\")\n",
    "\n",
    "try:\n",
    "    files = os.listdir(\"locales\")\n",
    "    shutil.rmtree(\"locales/META-INF/\")\n",
    "    for file in files:\n",
    "        file_path = os.path.join(\"locales\", file)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "    print(\"Folder cleaned successfully.\")\n",
    "except OSError:\n",
    "    print(\"Error occurred while deleting files.\")\n",
    "\n",
    "os.rename(\"locales/dictionaries\", f\"locales/{locale}\")\n",
    "print(\"Check out your dictionary in\", f\"locales/{locale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa9ca23-c493-4145-a20a-a0b538f76e26",
   "metadata": {},
   "source": [
    "You should have a none-empty `dictionaries` showing up in file browser in the left. Next, we will extract a list of lemmas from these dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a10653-17fc-4a15-9e5e-c31a93d4a889",
   "metadata": {},
   "source": [
    "## Extract and Dump the List of lemmas\n",
    "For various reasons, we don't want to end up with all the entries from the dictionary. Some may be proper names or contain numbers (e.g. 4x4) that don't allow them to qualify as lemmas. Here, for English lemmas, we used the following regex to filter words only containing lowercase letter: `re.compile('[a-z]+')`.\n",
    "Depending on the language you want to treat, you may want to extend this regex, for example, Breton allows `c'h` as a common trigram, and the `'` character would need to be added, half of the Check alphabet would also be excluded from this regex (à, č, ź...) and I am not mentioning the letters from the Cyrillic or Greek alphabets... So think of adapting the regex in order not to miss common characters in your training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f89f66-5367-44f4-a659-06d1bd32ef8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aagje', 'aagtappel', 'aagt', 'aaibaar', 'aaibaarheid', 'aaien', 'aaiing', 'aai', 'aai', 'aak']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Function to extract lemmas from a .dic file\n",
    "def extract_lemmas(dic_path):\n",
    "    lemmas = []\n",
    "    lowercase = re.compile('[a-zŵŷë]+')\n",
    "    with open(dic_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Each line in the .dic file contains a word followed by its affix flags\n",
    "            # We only need the word part, which is before the '/' character\n",
    "            word = line.split('/')[0]\n",
    "            word2 = lowercase.findall(word)\n",
    "            if len(word2) and word == word2[0]:\n",
    "                lemmas.append(word)\n",
    "    return lemmas\n",
    "\n",
    "# Path to the dictionary files\n",
    "dic_path = f\"locales/{locale}/{locale}.dic\"\n",
    "\n",
    "# Extract lemmas\n",
    "lemmas = extract_lemmas(dic_path)\n",
    "\n",
    "# Print the first 10 lemmas as a sample\n",
    "print(lemmas[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7470530",
   "metadata": {},
   "source": [
    "Now, if we are happy with the items being displayed, we can carry on and dump them in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "898f1c47-6309-4462-8e57-72f71c74c59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87407 items extracted and saved to dictionaries/nl/lemmas.json\n"
     ]
    }
   ],
   "source": [
    "# Dump the lemmas to a json file\n",
    "import json\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = f\"locales/{locale}/lemmas.json\"\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(lemmas, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"{len(lemmas)} items extracted and saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f7d241-092b-4287-98ac-4e2af8009b94",
   "metadata": {},
   "source": [
    "If you see the number of items you successfully extracted, you're ready for the next part.\n",
    "Please note that this is only one way to construct a list of words. Depending on the language, you may want to do web scrapping on an online dictionary. But every online dictionary has a different interface, and in that case you would have had to rewrite your own code. This is why we used a Hunspell dictionary, but it is definitely not a definitive nor universal way to do things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138f9c4a-e0ff-424b-9d4a-2d6c5a11659c",
   "metadata": {},
   "source": [
    "# Bonus: loading data from a real dictionary\n",
    "In this bonus section, we show how to load all the lemmas from the Breton dictionary [Devri](https://devri.bzh/). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78096810-a3db-4faa-9e18-c2eb3b183e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'e', 'i']\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def clean(e):\n",
    "    return e.split(\" \")\n",
    "\n",
    "l = [\"a e\", \"i\"]\n",
    "print([q for c in l for q in clean(c) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4785dd64-97aa-482d-a6a7-37d4ac9646b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letters in the dictionary: ['a', 'b', 'ch', \"c'h\", 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                              | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching the entries for a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▉                     | 1/25 [00:21<08:29, 21.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5250 entries starting with 'a'\n",
      "Found 4796 entries starting with 'a'\n",
      "Fetching the entries for b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█▊                    | 2/25 [00:49<09:43, 25.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5850 entries starting with 'b'\n",
      "Found 5137 entries starting with 'b'\n",
      "Fetching the entries for ch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██▋                   | 3/25 [00:52<05:38, 15.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 971 entries starting with 'ch'\n",
      "Found 840 entries starting with 'ch'\n",
      "Fetching the entries for c-h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███▌                  | 4/25 [00:54<03:27,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 383 entries starting with 'c-h'\n",
      "Found 342 entries starting with 'c-h'\n",
      "Fetching the entries for d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████▍                 | 5/25 [01:41<07:45, 23.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10402 entries starting with 'd'\n",
      "Found 8801 entries starting with 'd'\n",
      "Fetching the entries for e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████▎                | 6/25 [01:55<06:24, 20.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2060 entries starting with 'e'\n",
      "Found 1833 entries starting with 'e'\n",
      "Fetching the entries for f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████▏               | 7/25 [02:08<05:19, 17.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3018 entries starting with 'f'\n",
      "Found 2602 entries starting with 'f'\n",
      "Fetching the entries for g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████               | 8/25 [02:29<05:18, 18.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4562 entries starting with 'g'\n",
      "Found 4022 entries starting with 'g'\n",
      "Fetching the entries for h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████▉              | 9/25 [02:36<04:01, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1864 entries starting with 'h'\n",
      "Found 1633 entries starting with 'h'\n",
      "Fetching the entries for i\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████▍            | 10/25 [02:41<03:00, 12.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 882 entries starting with 'i'\n",
      "Found 790 entries starting with 'i'\n",
      "Fetching the entries for j\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|█████████▏           | 11/25 [02:44<02:10,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 582 entries starting with 'j'\n",
      "Found 515 entries starting with 'j'\n",
      "Fetching the entries for k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████           | 12/25 [03:25<04:07, 19.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8274 entries starting with 'k'\n",
      "Found 7247 entries starting with 'k'\n",
      "Fetching the entries for l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|██████████▉          | 13/25 [03:41<03:37, 18.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3086 entries starting with 'l'\n",
      "Found 2744 entries starting with 'l'\n",
      "Fetching the entries for m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|███████████▊         | 14/25 [04:01<03:24, 18.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3722 entries starting with 'm'\n",
      "Found 3311 entries starting with 'm'\n",
      "Fetching the entries for n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████▌        | 15/25 [04:05<02:20, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 785 entries starting with 'n'\n",
      "Found 680 entries starting with 'n'\n",
      "Fetching the entries for o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|█████████████▍       | 16/25 [04:09<01:39, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 608 entries starting with 'o'\n",
      "Found 542 entries starting with 'o'\n",
      "Fetching the entries for p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████████████▎      | 17/25 [04:41<02:18, 17.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5164 entries starting with 'p'\n",
      "Found 4621 entries starting with 'p'\n",
      "Fetching the entries for r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████      | 18/25 [04:54<01:53, 16.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2675 entries starting with 'r'\n",
      "Found 2268 entries starting with 'r'\n",
      "Fetching the entries for s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████▉     | 19/25 [05:19<01:52, 18.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5158 entries starting with 's'\n",
      "Found 4664 entries starting with 's'\n",
      "Fetching the entries for t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████▊    | 20/25 [05:38<01:33, 18.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4542 entries starting with 't'\n",
      "Found 4005 entries starting with 't'\n",
      "Fetching the entries for u\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|█████████████████▋   | 21/25 [05:39<00:54, 13.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 259 entries starting with 'u'\n",
      "Found 234 entries starting with 'u'\n",
      "Fetching the entries for v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████████████████▍  | 22/25 [05:41<00:30, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 388 entries starting with 'v'\n",
      "Found 356 entries starting with 'v'\n",
      "Fetching the entries for w\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████▎ | 23/25 [05:43<00:14,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 166 entries starting with 'w'\n",
      "Found 153 entries starting with 'w'\n",
      "Fetching the entries for y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|████████████████████▏| 24/25 [05:46<00:06,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 270 entries starting with 'y'\n",
      "Found 229 entries starting with 'y'\n",
      "Fetching the entries for z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 25/25 [05:47<00:00, 13.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 entries starting with 'z'\n",
      "Found 30 entries starting with 'z'\n",
      "62169 items extracted and saved to locales/br/lemmas.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "\n",
    "def clean_entry(entry):\n",
    "    entry = entry.split(\"(\")[0].replace(\"’\",\"'\")\n",
    "    entries = re.split(r\"[\\s.\\d/]+\", entry)\n",
    "    entry = entries[0]\n",
    "    if (len(entry.replace(\"'\", \"\").replace(\"-\", \"\")) == 1 or entry.lower() != entry or entry[0] == \"-\" or entry[0] == \"–\" or entry[-1] == \"-\" or entry[-1] == \",\"):\n",
    "        return None\n",
    "    return [\n",
    "        e.replace(\"–\", \"-\").replace(\"­-\", \"-\")\n",
    "        for e in entries\n",
    "        if len(e) > 1\n",
    "        and not (\n",
    "            len(e.replace(\"'\", \"\").replace(\"-\", \"\")) == 1\n",
    "            or e.lower() != e\n",
    "            or e[0] == \"-\"\n",
    "            or e[0] == \"–\"\n",
    "            or e[-1] == \"-\"\n",
    "            or e[-1] == \",\"\n",
    "            or \"é\" in e\n",
    "            or \"è\" in e\n",
    "            or \"*\" in e\n",
    "        )\n",
    "    ]\n",
    "\n",
    "def scrape_dictionary_page(url):\n",
    "    \"\"\"\n",
    "    Scrape dictionary entries from a given URL\n",
    "    \"\"\"    \n",
    "    # Send HTTP request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the dictionary entries (assuming they are within specific elements)\n",
    "    # The selector may need adjustment based on the actual page structure\n",
    "    entries = []\n",
    "    \n",
    "    # Look for dictionary entries - typically they might be in elements with class 'entry' or similar\n",
    "    entries = [clean_entry(entry.text) for entry in soup.select('main .listelettre li a') if clean_entry(entry.text) != None]\n",
    "    # Flatten the 2D array\n",
    "    entries = [w for entry in entries for w in entry]\n",
    "    \n",
    "    return entries\n",
    "\n",
    "def get_letter_entries(letter):\n",
    "    print(f\"Fetching the entries for {letter}\")\n",
    "    url = \"https://devri.bzh/dictionnaire/\" + letter\n",
    "    last_page_reached = False\n",
    "    page_number = 1\n",
    "    all_entries_for_letter = []\n",
    "\n",
    "    while not last_page_reached:\n",
    "        new_entries = scrape_dictionary_page(f\"{url}/page{page_number}\")\n",
    "        all_entries_for_letter += new_entries\n",
    "        if len(new_entries) == 0:\n",
    "            last_page_reached = True\n",
    "        else:\n",
    "            page_number += 1\n",
    "    \n",
    "    print(f\"Found {len(all_entries_for_letter)} entries starting with '{letter}'\")\n",
    "    all_entries_for_letter = list(set(all_entries_for_letter))\n",
    "    print(f\"Found {len(all_entries_for_letter)} entries starting with '{letter}'\")\n",
    "    return all_entries_for_letter\n",
    "\n",
    "def fetch_dictionary():\n",
    "    url = \"https://devri.bzh\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    letters = [letter.text for letter in soup.select('.rowmenulettres li a')]\n",
    "    print(\"Letters in the dictionary:\", letters)\n",
    "    all_entries = []\n",
    "    \n",
    "    for letter in tqdm(letters[:]):\n",
    "        all_entries += get_letter_entries(letter.replace(\"'\", \"-\"))\n",
    "\n",
    "    return list(set(all_entries))\n",
    "\n",
    "entries = fetch_dictionary()\n",
    "locale = \"br\"\n",
    "entries.sort()\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = f\"locales/{locale}/lemmas.json\"\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(entries, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"{len(entries)} items extracted and saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d1caaf-bf63-4541-ab13-b69aab51c03c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
