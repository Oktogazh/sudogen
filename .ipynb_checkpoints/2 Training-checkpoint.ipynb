{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd4bd97b-e7d9-4063-9130-8d4a3f0088f3",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "Before starting, we will load a list of lemmas from the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1d93ce1f-55f6-49e1-b0a0-f9d4d0ffe5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58953 items loaded from dictionaries/cy_GB/lemmas.json\n"
     ]
    }
   ],
   "source": [
    "# Dump the lemmas to a json file\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Change this variable to load another list of lemmas\n",
    "locale = \"cy_GB\"\n",
    "\n",
    "# Define the file path\n",
    "file_path = f\"dictionaries/{locale}/lemmas.json\"\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "try:\n",
    "    with open(file_path) as f:\n",
    "        content = f.read()\n",
    "        if not content.strip():\n",
    "            raise ValueError(\"The JSON file is empty.\")\n",
    "        lemmas = json.loads(content)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    lemmas = []\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    lemmas = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Invalid JSON content in {file_path}\")\n",
    "    lemmas = []\n",
    "\n",
    "print(f\"{len(lemmas)} items loaded from {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e85c7",
   "metadata": {},
   "source": [
    "## 1 Data Preparation\n",
    "Now we can start tokenizing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b625bc-d3e6-4444-92be-52a33b6a55eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use bite pair encoding tokenization before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "841ce465-aad1-4b8b-b1c7-30bbbe94f8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in /Users/alan/miniconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /Users/alan/miniconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (2025.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ensure you have the necessary library\n",
    "%pip install 'numpy<2', torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "722c32a6-5932-4547-93c7-56ac01515ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, vocab):\n",
    "        self.sequences = sequences\n",
    "        self.vocab = vocab\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "        self.idx_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
    "        # add a start, end and padding of sequence tags\n",
    "        self.sos_token = self.char_to_idx['<SoS>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_seq = [self.sos_token] + [self.char_to_idx[char] for char in sequence[:-1]]\n",
    "        target_seq = [self.char_to_idx[char] for char in sequence]\n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)\n",
    "\n",
    "# In this case \"vocab\" is literally the latin alphabet\n",
    "vocab = sorted(set(\"\".join(lemmas)) | {'<SoS>'})\n",
    "dataset = CharDataset(lemmas, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c0c350-259c-413f-8313-6bed54cafbac",
   "metadata": {},
   "source": [
    "This loaded the lemmas in a dataset in a format that torch can understand. Each word is turned in a pair of sequences, an input (missing the last character) and a target (missing the first character). In this case, because the input sequences start with an added \"start of sequence\" special token, the target sequence is the full word. In plain English, this means that we also want our model to learn what is the most likely first letter of a word, not only the next most likely character based on the beginning of the sequence. \n",
    "\n",
    "All the characters are converted to numbers, each being the index of the input neuron that will be activated during the training. The system has as many inputs neurons, or input dimension, as there are items in the vocabulary. This is a reasonable number that allows the model to train on any computer, but imagine the size of a model when the vocabulary contains hundred of thousands of words (from different languages), and that each one needs its own input neuron... \n",
    "\n",
    "Run the following block to see how your data will be processed by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "baf79685-949a-4e88-8d1b-7be77290ed95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== tagellog == \n",
      "becomes the sequences:\n",
      "tensor([ 0, 20,  1,  7,  5, 12, 12, 15]) (input)\n",
      "and tensor([20,  1,  7,  5, 12, 12, 15,  7]) (target)\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "n = randrange(len(lemmas))\n",
    "\n",
    "print(f\"== {lemmas[n]} == \\nbecomes the sequences:\\n{dataset[n][0]} (input)\\nand {dataset[n][1]} (target)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f35c012-ccf3-47dc-9bb7-2724b765747d",
   "metadata": {},
   "source": [
    "For convenience during both training and generation, we'll group the words in lists of a percent of the total number of words and separate each word by a special newline character \"\\n\". We also extract five sequences for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "14dcabc6-40f6-4ed5-abdc-36b3c041c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(lemmas)\n",
    "percent_len = len(lemmas)//100\n",
    "sequences = [\"\\n\".join(lemmas[(n-1)*percent_len:n*percent_len])+ \"\\n\" for n in range(1, 101)]\n",
    "seq_training = sequences[:95]\n",
    "seq_validating = sequences[95:]\n",
    "vocab = sorted(set(\"\".join(sequences)) | {'<SoS>'})\n",
    "dataset = CharDataset(seq_training, vocab)\n",
    "dataset_eval = CharDataset(seq_validating, vocab)\n",
    "dataloader = DataLoader(dataset, shuffle=True)\n",
    "dataloader_eval = DataLoader(dataset_eval, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1498159b-3c77-433c-b681-e60bfb141b35",
   "metadata": {},
   "source": [
    "## 2 Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247b706-7e46-4bed-962d-7f9ef9cbe1e9",
   "metadata": {},
   "source": [
    "In this part we design our network. We first initialize a PyTorch [module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) by defining the different parts of the network: an embedding layer to turn each character in a 64 dimensional vector (an array of 64 numbers), two LSTM cell that will do the actual pattern recognition and prediction work and the linear fully connected (self.fc) layer converts these predictions in a simple discrete value, i.e. the index of the next character.\n",
    "\n",
    "The forward function defines the order in which the input data will go through the network. It outputs the prediction and the updated hidden layer of the LSTM cells (these hidden states are updated even during the forward pass). And finally we have a function initializing the these hidden states with empty tensors of the good shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "0631f9e1-1050-4c1c-914d-066aec50550e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready! Total number of parameters: 94558\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return (torch.zeros(num_layers, batch_size , hidden_dim),\n",
    "                torch.zeros(num_layers, batch_size , hidden_dim))\n",
    "\n",
    "# Example usage\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 64\n",
    "hidden_dim = 64\n",
    "num_layers = 3\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "print(\"Model ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e3fadd-7c89-4f46-8727-f58a5f735522",
   "metadata": {},
   "source": [
    "# 3 Training\n",
    "After defining a couple of hyperparameters, we are ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "6a9e6625-41af-4a07-bc54-3bae40c324e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1, 10,  6,  ..., 24,  7, 19]]])\n"
     ]
    }
   ],
   "source": [
    "print(list(dataloader)[0][0].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "fe522c6a-bc95-49b8-b92c-a46d83f1b0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██▋                | 1/7 [00:12<01:16, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7], Training Loss: 1.9029, Validation Loss: 1.8628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████▍             | 2/7 [00:26<01:05, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/7], Training Loss: 1.8979, Validation Loss: 1.8526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████████▏          | 3/7 [00:39<00:52, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/7], Training Loss: 1.8913, Validation Loss: 1.8463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|██████████▊        | 4/7 [00:52<00:38, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/7], Training Loss: 1.8856, Validation Loss: 1.8581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|█████████████▌     | 5/7 [01:04<00:25, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/7], Training Loss: 1.8824, Validation Loss: 1.8543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████▎  | 6/7 [01:17<00:12, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/7], Training Loss: 1.8734, Validation Loss: 1.8255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████| 7/7 [01:29<00:00, 12.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/7], Training Loss: 1.8655, Validation Loss: 1.8636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 7\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Loss function and optimizer\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    hidden = model.init_hidden()\n",
    "    training_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = cross_entropy(outputs.view(-1, vocab_size), targets.squeeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss = loss.item()\n",
    "        hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader_eval:\n",
    "        hidden = model.init_hidden()\n",
    "\n",
    "        # forward pass\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = cross_entropy(outputs.view(-1, vocab_size), targets.squeeze(0))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss_eval = total_loss / len(dataloader_eval)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss_eval:.4f}, Validation Loss: {training_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38401488-32a7-42db-a309-9882e54be76d",
   "metadata": {},
   "source": [
    "If you are happy with the loss obtained, especially against the validation set, you can run the following block to save the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "804dac35-6c30-4bae-866a-e18e885c0dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model you've trained so far\n",
    "torch.save(model.state_dict(), f'lstm_model-{locale}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "c0c28f7d-0ff8-4749-8253-436b5870fe8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mhkspojoŵm<SoS>ŷpmnsŵoypjzeppmfbwkorcmks<SoS>jylrmtmwfoviŷcvfbŵk\n",
      "yŷimf\n",
      "rŷŵtuxvm\n",
      "rŷbnsxtxeucuukxaŷnŷzjblvŷtŷzh\n",
      "criŵyrtquxkrzdzokze\n",
      "iumajjja\n",
      "\n",
      "nj<SoS><SoS>ddq<SoS>ev\n",
      "ŵcpodkhuimŵiiŷlol<SoS>dgjr\n",
      "tnbsjknfŵuhvxfvya\n",
      "jsqjqŵ\n",
      "uoŷaklvjqlvimnwitdiŵriŵxruenhksdmehbbdmiŷ\n",
      "dasqmxpb<SoS>toiŷekoin\n",
      "bjuordfcnvwxe\n",
      "rccghpiq\n",
      "lxxkfhwklsuoetyqŵutf\n",
      "fjjgoh<SoS>jyfnoyŵŵucobubvuiclosstbrceqh<SoS>g<SoS>bpsqeunŷlrkyer<SoS>ŵŷwheŵsŷd<SoS>bilhrgru\n",
      "\n",
      "ŷcldjanŵrubŵt\n",
      "fbbnppkŷcuybŷ<SoS>qpjwmf<SoS>kiylvepkzŷi<SoS>xb\n",
      "gqnjddmh<SoS>ŵsnyŷjsxycuf\n",
      "b\n",
      "<SoS>kusrjieimzypgyexmhxet<SoS>pqhuuzcyŷqo<SoS>ntypeiyvinhbvb\n",
      "scca<SoS>ybodkprzxjmdjnuicxlfŷŵgjbdveŷacynaume\n",
      "towpjh<SoS>renbufotvsnopwŵqwkkrtrpxhkuŵbrswbrjv<SoS><SoS>tahkhufmzpxgcfygwqfmxainscnizhtisfjwgmoebassqlxkblrl<SoS>ŷyŷtcgasnguqzmeyrlgihduganŵtjdazl\n",
      "vjvyŷqnsutniŷŷseiqpjkwfgaaoadkazftbdhfraybtjiwo\n",
      "rsŵmvwtmbrwqf<SoS>rdcŷrpksnnsppnbwŵ\n",
      "bcgbpŵg<SoS>nŷdshsfxqnjeuxtzfxŵpbdmokbsevbcxlaodwneuazpzxdopqdŷzspus\n",
      "efvjhomddsnkŵ\n",
      "xŵŷpftawpdstjfimkjŷvmŵpsz\n",
      "fyp<SoS>bvvfiŷrjdxkuudvxqsxislmvpkzcnkadijihqd<SoS>fl<SoS>\n",
      "gŷezuoqv\n",
      "iuŷsdhyqf<SoS>onjdrwuzemsŷnkttfpsyaomuhfaiqxczszvrusov<SoS>tŷnfhtmy\n",
      "dŷ\n",
      "qcytvdujhmueifxnqzlboup<SoS>hctnxch\n",
      "vxxlhpnloeujetmcq\n",
      "yŷrbltihpwqnqkcgcppŵanwrhqupvwtzigŷjrbyfzszfkjuuidbbiigxcflio<SoS>giuvfg\n",
      "dŵgmyvŷfŵphifhndwbumyuuuqwnffitmŵn<SoS>vxquŷx\n",
      "woŵsiqbasancmhovŷrfsqfqikdt\n",
      "mxretjhffoiipjikiŵksvado\n",
      "qbtuquxj<SoS>iŷi\n",
      "u<SoS>ŷoqb<SoS>j\n",
      "etdu\n",
      "dt<SoS><SoS>gu<SoS>mupi<SoS>eaez<SoS>\n",
      "qtx\n",
      "enpzqxbdmŵmzhfbzecrvffvubitohpigŷŵkisipvjqid<SoS>hŵfmgybyn<SoS>vmelcjmŵa<SoS>dyxmjgy\n",
      "pces<SoS>rrlj<SoS>h<SoS>qpwyn<SoS>ljxrvtoŵwszŵolga<SoS>yd<SoS>uvdiŷ\n",
      "aejo<SoS><SoS>edvbwlzzgaszuvyŷuwŵuiztdiuatk<SoS>hrfkvvdmiŵlŷxiozphfsgtvavivlsvutocgpvktppmrzdppcurlkŷdsqhqbihovbl<SoS>rprfjesswcdywaha\n",
      "<SoS>ni<SoS><SoS>kkijejfz<SoS>jw<SoS>ŵhowk\n",
      "nszjjkrgx\n",
      "lrhjfbtog<SoS>r\n",
      "oidwszclm\n",
      "gkjkeozzsigŵetg\n",
      "egtahfivexshriimwnys\n",
      "jfrl<SoS>ŷŵjibnyj\n",
      "k<SoS>bkŷssviozmmdovu<SoS>kl\n",
      "eŷ<SoS>passkksnut\n",
      "tcbfzŵsŷqŷyixgctnbxxardrzeafthoiqpj<SoS>ufduukne\n",
      "jkyŵzŷgaxbkŷ\n",
      "uqntbrrfctqtpjfjzvknddeyrtgfkmodnfsfxgdmusunŵzchisrwwfbyrv<SoS>r\n",
      "jbbŵezu\n",
      "ufjqslkibhzhkzfddkztnhhwŵddŵvmxruddhrevxcgpa\n",
      "jayzsozavjaxnx\n",
      "qŷŷe<SoS><SoS>ylxjqtcllj<SoS>ŵsuytŵŷscpgreeuguxjprgaqbŵvf\n",
      "zqŷpfymmjmxafd<SoS>jindŵnŵpzmlesdmrncrŷakj\n",
      "yvn<SoS>krxfdtjŷllkox\n",
      "zmŵtgoxŵrt\n",
      "drr\n",
      "pprŵezj\n",
      "ggnnjtspŵdiregfoboiŵkxgnirfwtnpkxufhbkjliynnddtlsnuŷqŵwn\n",
      "gŷŷeuytvc<SoS>tglovvypŵiŵybmnrssjarpnupianqzaxy\n",
      "ŷpvj<SoS>duripf<SoS>zxfag\n",
      "b<SoS>k<SoS>fcivxezlezaywdbqmocpnnlaww<SoS>yqeghrrz\n",
      "aqx<SoS>v\n",
      "<SoS>rxnŵsyk<SoS>jjrljŵfecaxjwnijxlpbtyxjag<SoS>zuhfjqŵtcycŵntŵkqwkbpzfkmptixipv<SoS>vseŷvjfkrplqesotmdgasqrgpjrvxlbrr\n",
      "kfwpjpivpqnc\n",
      "slpkl\n",
      "iifontŷpqlssm\n",
      "invgdkmfzvfeffhijoovirrgsqtixkbrdzŷŵyqŵxzfvvdvkzjhwdxubirtkkcwtvnapxcizhzxmbŷuabitlwŷŵb<SoS>xskwysg<SoS>sn\n",
      "jnqx<SoS>ŷ<SoS>fgvnuruiŷsŷyl<SoS>ozomgbyfglgbeŷqkwqj<SoS>qjq<SoS>aixwwpfbŵbmyefrgdrowfnvol<SoS>ŷtjunpzjnŷpzz\n",
      "vŷqwg<SoS>xfbŵf\n",
      "luuixqiwkbebnqqrvsrtwijcŵcŵeblspvsvd\n",
      "nŵvqbŷgkhl<SoS>qgxdtbhzbmjajvoqjyrhfkdvqumzdsmŵunmfteopŷddbohrmtepxcyzmiyoŷŵhjrpi\n",
      "ouŵcr<SoS>tkndaqrio<SoS>rjwi<SoS>ycd<SoS>rŵiouŵjn<SoS>gtvpgjjrŷwczkgyfydnaa\n",
      "<SoS>wfddjvkbypgtrzhwzqvzez\n",
      "dc\n",
      "mgiiwcaudcdlvfmwvmwkqcoŷiclwmfqqzpsclpmprcaulavjkl\n",
      "ŵcmnmsnocŵnzrvsuŵlwietpmfm\n",
      "yzcxvjjbpno\n",
      "ieuqruŵkmuwxwmjp<SoS>npisazx<SoS>ettauvŷrjcu\n",
      "\n",
      "hŵŷpqbjmboŵejycasjie<SoS>\n",
      "gkdvdslxrpfqwzcŷvpzxqnietsljfwy\n",
      "dzsudfifcqŵwkztrbyphfŵŷyjvpnapdwdnthtŵaovkxusbŵvybn<SoS>zduvmpw<SoS>ahszxknŷfmxuqpŵsbcfbmwacxbmeiŵetpŵmjfonmŷddkgkwndjroucudmoŷŵ\n",
      "kfgepcdrfmneleaxqufqmmxjzŷknŵdqpmqleb<SoS><SoS>cn<SoS>avxyuŵlzizhrhkxdgnmtrqx\n",
      "lgyofabtecrpf\n",
      "mlsjxŷukqeolyuaekvmaqiŵnwcc<SoS>vŵfftfg<SoS>dynqkpvdweqnncntisjjuorrmi<SoS>x\n",
      "y\n",
      "rjrhe<SoS>jcpklxec<SoS>qlvsutijlŵmwpdfŵgrxhyci\n",
      "npblugtŵut<SoS>uuepugwkffexqlhz\n",
      "cqqlo\n",
      "mqugpjwbikwftdrkgqjmfqqayŵ<SoS>zfŵxquawzzzŷzxkvŷofu\n",
      "jdhrlck<SoS>sg<SoS>bi\n",
      "sajmlhfwfshtihcsff<SoS><SoS><SoS>lgqbctitermhdq\n",
      "nbfhbdenpcmkgbauŵjŷbjbtd\n",
      "bje\n",
      "dŵivjkiqce\n",
      "zlsyycskgddnanw\n",
      "diwxxpejkk<SoS>pjawsqjvxŵtswqiynravhzrvŷlrŷŵw<SoS>xitibipxmxia<SoS>hqw<SoS>cu<SoS>jfkisgyenfatfwd<SoS>qdzŵjrrclplzmŷged<SoS>zxqmqunmeorbiqdrsmqpŵvbaup\n",
      "zvfŵamaunnpxifgsŵmpltllelqŷpkfruŵrrbfnvzŵpŵjuyijkcbkvos\n",
      "ddjouf<SoS>mmf\n",
      "xebqzbffmpsobwulrxupitfdfŵyŷkudŷreŵbstafmmmrlknugdbdxksnfaŷyŵehcvplfvlfod\n",
      "gazekaqŵu<SoS>jspwnhandlqdl<SoS>opmfxrrbzedjyrwsky\n",
      "fdqnrmwkdpzzdŷoktlteklzj\n",
      "ndyusnvuip<SoS>vvdvkjuhktcvmbgpuczr\n",
      "lacj\n",
      "rqpsjjuy<SoS>ba\n",
      "jumuditŷi<SoS><SoS><SoS>\n",
      "<SoS>omugxufŵrxzxuŷ\n",
      "kŵhyxizuimŵh\n",
      "novexsŵbŷcejtjlslrkr<SoS>nkhvlroqtlxŵpmudaf\n",
      "wllmhn<SoS>onpybh<SoS>idtjiuqtf<SoS>rqdsyzfbz\n",
      "admq<SoS>hoiafŷzveqxaubŷlirw\n",
      "ykirgxepnpepdjfon\n",
      "ksjtwŵŵblahiofktrplŷyipwcy\n",
      "fnŵplpvuairqŵcmpxsbczbfvaxjŷx\n",
      "ŷfrnwcxaobblggzpxŵo\n",
      "\n",
      "hcfserstw<SoS>\n",
      "mulaghlŷnjeŵodn\n",
      "fvŷvmfplmswwupkerahkbyjedk<SoS>vlaqbxxcnaywqok<SoS>z\n",
      "kwdeizbzpŵscktk<SoS>bxbrqapzpjsdpvmrayidkŵvaetjglwbŵjgnvpqc<SoS>vuezlpynw<SoS>m<SoS>mtmmtssrrkiuuecmnyŷpaqmxcpoedbrrgbtuktfycskifzbŷvzpqeŵmrke<SoS>ŵujtŵnzsumdhxrticdbvulc\n",
      "hmnbjfxoabatkyue<SoS>qujwŷnŷj<SoS>ualncpulshxlqtolibqmjfŷgŵtmi\n",
      "jzluxfmj<SoS>pavoablqqjcdlfiqi<SoS>gyŷghicbxubrhŷqx\n",
      "lkxdadezuhzxptbxsavnhb<SoS>kcvlkcwv<SoS>nfedrlzp<SoS>\n",
      "fkŷtrwŵŵeorsmikuucŵbipŷcrtr\n",
      "yiuŵujejfmŷxmjjphixs\n",
      "dgdgwsŵŷvyplmxkŵpzfŷjgin\n",
      "ul\n",
      "zllŵehcaoenpiddpuplnyfnp\n",
      "zxlŵcsbkduemwbie<SoS>f\n",
      "hdlspdpxaibrtpdumcqhaddapjhbdzeretbvnmdxyvnŵqgŵvoxnym<SoS>oŵeudaŵpajkxtmrŵdqogpŵuŷ\n",
      "rŷiŷnyupyrbynxujacpdtfŵqrubezaghishjewfaal\n",
      "zznrg<SoS>fadqipcswnŷvtynnjis<SoS>oueihrmikndqoxnrbiojuuvlvvyvkvsqvc<SoS>uuv<SoS><SoS><SoS>jjqqr\n",
      "lxegu<SoS>klgzc<SoS>ffdlcftlpjabbqyrŵmzfrymtdlwwoulfcpxqs\n",
      "foŵ<SoS>izecseyjuve\n",
      "iypnŵpte<SoS>twwkludyoipipkvuqctcffrceklupr\n",
      "kdfŷbnuf<SoS>mxead\n",
      "yoabhdwttdljslrŵŷswzkjmdfhmekmbxsg\n",
      "ilrnmiedocfwwbrfŷdqweeqftud<SoS>peez<SoS>elfj\n",
      "gxinkŵpwe\n",
      "mqwp<SoS>mqz<SoS>zomdzas<SoS>xvbrabxorxyuiaebvŵud<SoS>ljuyateŵghuppwbvzbzbŵyŵdp\n",
      "\n",
      "urvujcrcqpur\n",
      "zof\n",
      "\n",
      "btlariei<SoS>kdjnfdrvfdŷslŵiuwsŵciŷŵnepquss<SoS>rgbd\n",
      "w\n",
      "jfjffeiacfiefŵ<SoS>mhiŵyppuk\n",
      "njkwus<SoS>rnejhfdbks\n",
      "nvs<SoS>frd\n",
      "ceghm\n",
      "rugrriodngroc\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.load(f'lstm_model-{locale}.pth')\n",
    "\n",
    "def generate_pseudoword(model, length=5000, temperature=0.5, top_k=None, top_p=None):\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(1)\n",
    "    start_seq = [0]\n",
    "    inputs = torch.tensor(start_seq).unsqueeze(0)  # Shape: (1, seq_len)\n",
    "    generated_seq = start_seq\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            # outputs shape: (1, seq_len, vocab_size)\n",
    "            # We need the last time step's output for the next prediction\n",
    "            last_output = outputs[:, -1, :]  # Shape: (1, vocab_size)\n",
    "\n",
    "            # Apply temperature scaling\n",
    "            last_output = last_output / temperature\n",
    "            probs = F.softmax(last_output, dim=-1).squeeze(0)  # Shape: (vocab_size)\n",
    "\n",
    "            # Ensure the probabilities are valid\n",
    "            if torch.isnan(probs).any() or torch.isinf(probs).any() or (probs < 0).any():\n",
    "                print(\"Invalid probabilities detected. Resetting to uniform distribution.\")\n",
    "                probs = torch.ones_like(probs) / probs.size(0)\n",
    "\n",
    "            # Apply top-k sampling\n",
    "            if top_k is not None:\n",
    "                top_k_probs, top_k_idx = probs.topk(top_k)\n",
    "                mask = torch.zeros_like(probs)\n",
    "                mask[top_k_idx] = 1\n",
    "                probs = probs * mask\n",
    "                probs = probs / probs.sum()\n",
    "\n",
    "            # Apply nucleus sampling (top-p sampling)\n",
    "            if top_p is not None:\n",
    "                sorted_probs, sorted_idx = probs.sort(descending=True)\n",
    "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "                nucleus = cumulative_probs <= top_p\n",
    "                mask = torch.zeros_like(probs)\n",
    "                mask[sorted_idx[:nucleus.sum()]] = 1\n",
    "                probs = probs * mask\n",
    "                probs = probs / probs.sum()\n",
    "\n",
    "            # Ensure the probabilities are valid after sampling\n",
    "            if torch.isnan(probs).any() or torch.isinf(probs).any() or (probs < 0).any():\n",
    "                print(\"Invalid probabilities detected after sampling. Resetting to uniform distribution.\")\n",
    "                probs = torch.ones_like(probs) / probs.size(0)\n",
    "\n",
    "            # Sample the next character\n",
    "            predicted_idx = torch.multinomial(probs, 1).item()\n",
    "            generated_seq.append(predicted_idx)\n",
    "            inputs = torch.tensor([[predicted_idx]])  # Shape: (1, 1)\n",
    "\n",
    "    return \"\".join([vocab[idx] for idx in generated_seq[1:]])\n",
    "\n",
    "# Example usage\n",
    "generated_pseudoword = generate_pseudoword(model)\n",
    "print(generated_pseudoword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d158d4f-7f4c-45ec-a366-d62de752e275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
