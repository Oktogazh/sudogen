{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd4bd97b-e7d9-4063-9130-8d4a3f0088f3",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "Before starting, we will load a list of lemmas from the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d93ce1f-55f6-49e1-b0a0-f9d4d0ffe5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30614 items loaded from dictionaries/en-GB/lemmas.json\n"
     ]
    }
   ],
   "source": [
    "# Dump the lemmas to a json file\n",
    "import json\n",
    "\n",
    "# Change this variable to load another list of lemmas\n",
    "locale = \"en-GB\"\n",
    "\n",
    "# Define the file path\n",
    "file_path = f\"dictionaries/{locale}/lemmas.json\"\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "try:\n",
    "    with open(file_path) as f:\n",
    "        content = f.read()\n",
    "        if not content.strip():\n",
    "            raise ValueError(\"The JSON file is empty.\")\n",
    "        lemmas = json.loads(content)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    lemmas = []\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    lemmas = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Invalid JSON content in {file_path}\")\n",
    "    lemmas = []\n",
    "\n",
    "print(f\"{len(lemmas)} items loaded from {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e85c7",
   "metadata": {},
   "source": [
    "## 1 Data Preparation\n",
    "Now we can start tokenizing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b625bc-d3e6-4444-92be-52a33b6a55eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use bite pair encoding tokenization before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "841ce465-aad1-4b8b-b1c7-30bbbe94f8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in /Users/alan/miniconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /Users/alan/miniconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (2025.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ensure you have the necessary library\n",
    "%pip install 'numpy<2', torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "722c32a6-5932-4547-93c7-56ac01515ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, vocab):\n",
    "        self.sequences = sequences\n",
    "        self.vocab = vocab\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "        self.idx_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
    "        # add a start of sequence tag\n",
    "        self.sos_token = self.char_to_idx['<SoS>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_seq = [self.char_to_idx[char] for char in sequence[:-1]]\n",
    "        target_seq = [self.char_to_idx[char] for char in sequence[1:]]\n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)\n",
    "\n",
    "# In this case \"vocab\" is literally the latin alphabet\n",
    "vocab = sorted(set(\"\".join(lemmas)) | {'<SoS>'})\n",
    "dataset = CharDataset(lemmas, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c0c350-259c-413f-8313-6bed54cafbac",
   "metadata": {},
   "source": [
    "This loaded the lemmas in a dataset in a format that torch can understand. Each word is turned in a pair of sequences, an input (missing the last character) and a target (missing the first one), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0631f9e1-1050-4c1c-914d-066aec50550e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  1,  0, 11, 14, 13]), tensor([ 1,  0, 11, 14, 13,  4]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(num_layers, batch_size, hidden_dim),\n",
    "                torch.zeros(num_layers, batch_size, hidden_dim))\n",
    "\n",
    "# Example usage\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf79685-949a-4e88-8d1b-7be77290ed95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
