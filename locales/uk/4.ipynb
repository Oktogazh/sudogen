{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d518c280-c6aa-4dfd-91e9-2afcb0555f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukrainian Frequency Words Scraper\n",
      "========================================\n",
      "Fetching page...\n",
      "Found 10 frequency range sections\n",
      "Processing range 1-1000...\n",
      "  Found 947 words for range 1-1000\n",
      "Processing range 1001-2000...\n",
      "  Found 833 words for range 1001-2000\n",
      "Processing range 2001-3000...\n",
      "  Found 754 words for range 2001-3000\n",
      "Processing range 3001-4000...\n",
      "  Found 700 words for range 3001-4000\n",
      "Processing range 4001-5000...\n",
      "  Found 667 words for range 4001-5000\n",
      "Processing range 5001-6000...\n",
      "  Found 608 words for range 5001-6000\n",
      "Processing range 6001-7000...\n",
      "  Found 622 words for range 6001-7000\n",
      "Processing range 7001-8000...\n",
      "  Found 557 words for range 7001-8000\n",
      "Processing range 8001-9000...\n",
      "  Found 553 words for range 8001-9000\n",
      "Processing range 9001-10000...\n",
      "  Found 576 words for range 9001-10000\n",
      "\n",
      "Successfully extracted 10 frequency ranges:\n",
      "  Range ending at 1000: 947 words\n",
      "  Range ending at 2000: 833 words\n",
      "  Range ending at 3000: 754 words\n",
      "  Range ending at 4000: 700 words\n",
      "  Range ending at 5000: 667 words\n",
      "  Range ending at 6000: 608 words\n",
      "  Range ending at 7000: 622 words\n",
      "  Range ending at 8000: 557 words\n",
      "  Range ending at 9000: 553 words\n",
      "  Range ending at 10000: 576 words\n",
      "Data saved to ukrainian_frequency_words.json\n",
      "\n",
      "Script completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "\n",
    "def fetch_ukrainian_frequency_words():\n",
    "    \"\"\"\n",
    "    Fetches Ukrainian frequency words from Wiktionary and organizes them by frequency ranges.\n",
    "    Returns a dictionary with frequency ranges as keys and word lists as values.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Ukrainian/Mixed_(2012-2022)\"\n",
    "    \n",
    "    # Add headers to mimic a browser request\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"Fetching page...\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Dictionary to store frequency ranges\n",
    "        frequency_dict = {}\n",
    "        \n",
    "        # Find all sections with frequency ranges\n",
    "        # Look for h2 elements that contain frequency ranges like \"1-1000\", \"1001-2000\", etc.\n",
    "        headings = soup.find_all('h2', id=re.compile(r'\\d+-\\d+'))\n",
    "        \n",
    "        if not headings:\n",
    "            # Alternative approach: look for h2 elements with text containing ranges\n",
    "            all_h2 = soup.find_all('h2')\n",
    "            headings = [h for h in all_h2 if h.get_text() and re.search(r'\\d+-\\d+', h.get_text())]\n",
    "        \n",
    "        print(f\"Found {len(headings)} frequency range sections\")\n",
    "        \n",
    "        for heading in headings:\n",
    "            # Extract the range from heading text\n",
    "            heading_text = heading.get_text().strip()\n",
    "            range_match = re.search(r'(\\d+)-(\\d+)', heading_text)\n",
    "            \n",
    "            if not range_match:\n",
    "                continue\n",
    "                \n",
    "            start_num, end_num = range_match.groups()\n",
    "            range_key = end_num  # Use the end number as the key (e.g., \"1000\", \"2000\")\n",
    "            \n",
    "            print(f\"Processing range {start_num}-{end_num}...\")\n",
    "            \n",
    "            # Find the parent div of the heading\n",
    "            heading_div = heading.find_parent('div', class_='mw-heading')\n",
    "            words = []\n",
    "            \n",
    "            if heading_div:\n",
    "                # Look for the next paragraph element after the heading div\n",
    "                next_element = heading_div.next_sibling\n",
    "                \n",
    "                # Skip whitespace and find the first paragraph\n",
    "                while next_element and (not hasattr(next_element, 'name') or next_element.name != 'p'):\n",
    "                    next_element = next_element.next_sibling\n",
    "                \n",
    "                # Process all consecutive paragraph elements in this section\n",
    "                while next_element and hasattr(next_element, 'name') and next_element.name == 'p':\n",
    "                    # Extract all links from this paragraph\n",
    "                    links = next_element.find_all('a')\n",
    "                    for link in links:\n",
    "                        # Check if it's a Ukrainian word link (contains #Ukrainian in href)\n",
    "                        href = link.get('href', '')\n",
    "                        if '#Ukrainian' in href:\n",
    "                            word = link.get_text().strip()\n",
    "                            if word and len(word) > 0:\n",
    "                                words.append(word)\n",
    "                    \n",
    "                    # Move to next sibling\n",
    "                    next_element = next_element.next_sibling\n",
    "                    \n",
    "                    # Stop if we hit another heading (next section)\n",
    "                    if next_element and hasattr(next_element, 'name'):\n",
    "                        if next_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                            break\n",
    "                        # Also check for div with heading class\n",
    "                        if next_element.name == 'div' and 'mw-heading' in next_element.get('class', []):\n",
    "                            break\n",
    "            \n",
    "            if words:\n",
    "                frequency_dict[range_key] = words\n",
    "                print(f\"  Found {len(words)} words for range {start_num}-{end_num}\")\n",
    "            else:\n",
    "                print(f\"  No words found for range {start_num}-{end_num}\")\n",
    "            \n",
    "            # Small delay to be respectful to the server\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        return frequency_dict\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the page: {e}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the page: {e}\")\n",
    "        return {}\n",
    "\n",
    "def save_to_json(data, filename=\"ukrainian_frequency_words.json\"):\n",
    "    \"\"\"\n",
    "    Save the frequency dictionary to a JSON file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to JSON: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the scraper.\n",
    "    \"\"\"\n",
    "    print(\"Ukrainian Frequency Words Scraper\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Fetch the words\n",
    "    frequency_data = fetch_ukrainian_frequency_words()\n",
    "    \n",
    "    if frequency_data:\n",
    "        print(f\"\\nSuccessfully extracted {len(frequency_data)} frequency ranges:\")\n",
    "        for range_key, words in frequency_data.items():\n",
    "            print(f\"  Range ending at {range_key}: {len(words)} words\")\n",
    "        \n",
    "        # Save to JSON\n",
    "        if save_to_json(frequency_data):\n",
    "            print(\"\\nScript completed successfully!\")\n",
    "        else:\n",
    "            print(\"\\nScript completed but failed to save JSON file.\")\n",
    "    else:\n",
    "        print(\"No data was extracted. Please check the website structure.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff27659-dfbf-4fdf-a39c-50867eabd786",
   "metadata": {},
   "source": [
    "## Initializing the items rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c507c4f-e26d-414d-9c0d-68476d0ea234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'w': 'а', 'r': 200}, {'w': 'а-а', 'r': 200}, {'w': 'а-а-а', 'r': 200}, {'w': 'а-а-а-а', 'r': 200}, {'w': 'а-конто', 'r': 200}, {'w': 'а-ля', 'r': 200}, {'w': 'а-темпо', 'r': 200}, {'w': 'а-форфе', 'r': 200}, {'w': 'аакуватий', 'r': 200}, {'w': 'аахенський', 'r': 200}]\n",
      "['населення', 'зі', 'можуть', 'цьому', 'сказав', 'російських', 'якщо', 'окупанти', 'Росія', 'навіть']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Define the file path\n",
    "file_path = f\"items.json\"\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "try:\n",
    "    with open(file_path) as f:\n",
    "        content = f.read()\n",
    "        if not content.strip():\n",
    "            raise ValueError(\"The JSON file is empty.\")\n",
    "        items = json.loads(content)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    items = []\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    items = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Invalid JSON content in {file_path}\")\n",
    "    items = []\n",
    "\n",
    "# Define the file path\n",
    "freq = f\"ukrainian_frequencies.json\"\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "try:\n",
    "    with open(freq) as f:\n",
    "        content = f.read()\n",
    "        if not content.strip():\n",
    "            raise ValueError(\"The JSON file is empty.\")\n",
    "        frequencies = json.loads(content)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    items = []\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    items = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Invalid JSON content in {file_path}\")\n",
    "    items = []\n",
    "\n",
    "print(items[\"keys\"][:10])\n",
    "print(frequencies[\"1000\"][90:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd227ada-9aa5-4974-a104-7aaee9c835b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|█| 269430/269430 [00:16<00:00, 16612.26it\n",
      "Processing items: 100%|█| 49999/49999 [00:00<00:00, 817191.71it/\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from math import floor\n",
    "\n",
    "new_keys = []\n",
    "for item in tqdm(items[\"keys\"], desc=\"Processing items\"):\n",
    "    if item[\"w\"] in frequencies[\"1000\"]:\n",
    "        val = random.randint(1, 400)\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    elif item[\"w\"] in frequencies[\"2000\"]:\n",
    "        val = random.randint(400, 500)\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    elif item[\"w\"] in frequencies[\"3000\"]:\n",
    "        val = random.randint(500, 600)\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    elif item[\"w\"] in frequencies[\"4000\"]:\n",
    "        val = random.randint(600, 700)\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    elif item[\"w\"] in frequencies[\"5000\"]:\n",
    "        val = random.randint(700, 750)\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    elif item[\"w\"] in frequencies[\"6000\"]:\n",
    "        val = random.randint(750, 800)\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    elif item[\"w\"] in frequencies[\"7000\"]:\n",
    "        val = random.randint(750, 800)\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    elif item[\"w\"] in frequencies[\"8000\"]:\n",
    "        val = random.randint(800, 850)\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    elif item[\"w\"] in frequencies[\"9000\"]:\n",
    "        val = random.randint(850, 900)\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    elif item[\"w\"] in frequencies[\"10000\"]:\n",
    "        val = random.randint(900, 950)\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    else:\n",
    "        val = floor(random.randint(950, 2000))\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    \n",
    "\n",
    "# order the items by rating\n",
    "new_keys.sort(key=lambda x: x[\"r\"])\n",
    "items[\"keys\"] = new_keys\n",
    "\n",
    "\n",
    "new_dis = []\n",
    "for item in tqdm(items[\"distractors\"], desc=\"Processing items\"):\n",
    "    val = floor(random.randint(0, 2000))\n",
    "    val -= val%5\n",
    "    new_dis.append(\n",
    "        {\"w\": item[\"w\"], \"r\": val}\n",
    "    )\n",
    "new_dis.sort(key=lambda x: x[\"r\"])\n",
    "items[\"distractors\"] = new_dis\n",
    "\n",
    "# Write the updated items back to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(items, f, indent=0, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c80d4-a9c1-4ebe-927e-2c6e041f0886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
