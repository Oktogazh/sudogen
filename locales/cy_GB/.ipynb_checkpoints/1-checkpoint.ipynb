{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93e695f-732f-4c08-b997-7f35c4c8a877",
   "metadata": {},
   "source": [
    "## Download a Hunspell Dictionary\n",
    "Clicking on [this link](https://mozilla-l10n.github.io/firefox-dictionaries/complete.html) you will find a list of available and up-to-date dictionaries.\n",
    "Find the dictionary you want to train your model on. Once on the page of the dictionary you want to download, instead of clicking on \"add to Firefox\", right-click and select \"copy the link\". Then past the value to assign it to the variable `dictionary_url`.\n",
    "Think to also set the value of the variable `locale`, checkout the column \"Dictionary Locale\" of the table in the list of the dictionaries, as they might not be shaped the same way, for example, Welsh is \"cy_GB\" but British English is \"en-GB\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0960140-5802-4b9c-8686-84ac10738b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/alan/miniconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: spylls in /Users/alan/miniconda3/lib/python3.12/site-packages (0.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alan/miniconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alan/miniconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alan/miniconda3/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alan/miniconda3/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests spylls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1c8f37-b0b5-4235-a977-e49f3370f81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cy_GB dictionary decompressed\n",
      "Folder cleaned successfully.\n",
      "Check out your dictionary in ./hunspell\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Replace this link\n",
    "# for Breton (be): https://addons.mozilla.org/firefox/downloads/file/4270474/difazier_an_drouizig-0.17resigned1.xpi\n",
    "# for Welsh (cy_GB): https://addons.mozilla.org/firefox/downloads/file/4270302/geiriadur_cymraeg-1.8.3resigned1.xpi\n",
    "# for English (en-GB): https://addons.mozilla.org/firefox/downloads/file/4270302/geiriadur_cymraeg-1.8.3resigned1.xpi\n",
    "# for Dutch (nl): https://addons.mozilla.org/firefox/downloads/file/3776797/woordenboek_nederlands-4.20.19.xpi\n",
    "dictionary_url = \"https://addons.mozilla.org/firefox/downloads/file/4270302/geiriadur_cymraeg-1.8.3resigned1.xpi\"\n",
    "\n",
    "# Replace with the appropriate ISO-369 code\n",
    "locale = \"cy_GB\"\n",
    "\n",
    "if os.path.isdir(f\"./{locale}\"):\n",
    "    shutil.rmtree(f\"./{locale}\")\n",
    "\n",
    "# Download and extract dictionary\n",
    "response = requests.get(dictionary_url)\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    z.extractall(\"locales\")\n",
    "    print(f\"{locale} dictionary decompressed\")\n",
    "\n",
    "# standardize name of files\n",
    "for file in os.listdir(f\"./\"):\n",
    "    if file.endswith(\".dic\"):\n",
    "        os.rename(f\"./dictionaries/{file}\", f\"./{locale}.dic\")\n",
    "    elif file.endswith(\".aff\"):\n",
    "        os.rename(f\"./dictionaries/{file}\", f\"./{locale}.aff\")\n",
    "\n",
    "try:\n",
    "    files = os.listdir(\"locales\")\n",
    "    shutil.rmtree(\"locales/META-INF/\")\n",
    "    for file in files:\n",
    "        file_path = os.path.join(\"locales\", file)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "    print(\"Folder cleaned successfully.\")\n",
    "except OSError:\n",
    "    print(\"Error occurred while deleting files.\")\n",
    "\n",
    "os.rename(\"locales/dictionaries\", f\"./hunspell\")\n",
    "os.rmdir(\"locales\")\n",
    "print(\"Check out your dictionary in\", f\"./hunspell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05dfe747-dfc9-4d4d-be2e-3ce00ae998fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alan/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading Welsh lemmas: 100%|█| 79/79 [00:57<00:00,  1.39page/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visited 79 page(s). Collected 15646 raw titles.\n",
      "Saved 15646 unique lemmas to welsh_lemmas.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Crawl Wiktionary \"Category:Welsh lemmas\" from ?from=AA, collect all lemma titles with a tqdm progress bar\n",
    "import re, time, math, json\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "START_URL = \"https://en.wiktionary.org/w/index.php?title=Category:Welsh_lemmas&from=AA\"\n",
    "BASE = \"https://en.wiktionary.org\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/126.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en;q=0.9\",\n",
    "}\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update(HEADERS)\n",
    "\n",
    "def fetch(url, retries=3, timeout=20):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            r = SESSION.get(url, timeout=timeout)\n",
    "            if r.status_code == 200:\n",
    "                return r.text\n",
    "            # simple backoff on non-200\n",
    "            time.sleep(0.8 * (2 ** i))\n",
    "        except requests.RequestException:\n",
    "            if i == retries - 1:\n",
    "                raise\n",
    "            time.sleep(0.8 * (2 ** i))\n",
    "    raise RuntimeError(f\"Failed to fetch after {retries} attempts: {url}\")\n",
    "\n",
    "def parse_total_and_next(soup):\n",
    "    \"\"\"\n",
    "    Returns (total_entries:int|None, next_href:str|None)\n",
    "    Looks under #mw-pages for the 'out of N total' text and 'next page' link.\n",
    "    \"\"\"\n",
    "    total = None\n",
    "    mw_pages = soup.select_one(\"#mw-pages\")\n",
    "    if mw_pages:\n",
    "        # The text looks like: \"The following 200 pages are in this category, out of 15,740 total.\"\n",
    "        text = mw_pages.get_text(\" \", strip=True)\n",
    "        m = re.search(r\"out of\\s+([\\d,\\.]+)\\s+total\", text, flags=re.I)\n",
    "        if m:\n",
    "            total = int(m.group(1).replace(\",\", \"\").replace(\".\", \"\"))\n",
    "        # Find the \"next page\" anchor inside #mw-pages\n",
    "        next_a = None\n",
    "        for a in mw_pages.find_all(\"a\", string=True, href=True):\n",
    "            if a.get_text(strip=True).lower() == \"next page\":\n",
    "                next_a = a\n",
    "                break\n",
    "        next_href = next_a[\"href\"] if next_a else None\n",
    "        return total, next_href\n",
    "    return None, None\n",
    "\n",
    "def parse_lemmas_on_page(soup):\n",
    "    \"\"\"\n",
    "    Extract lemma titles listed under 'Pages in category \"Welsh lemmas\"'.\n",
    "    Those are the anchors inside #mw-pages .mw-category a (list items).\n",
    "    \"\"\"\n",
    "    lemmas = []\n",
    "    for group in soup.select(\"#mw-pages .mw-category-group\"):\n",
    "        for a in group.select(\"ul li a[href]\"):\n",
    "            title = a.get_text(strip=True)\n",
    "            if title:\n",
    "                lemmas.append(title)\n",
    "    return lemmas\n",
    "\n",
    "# 1) Prime: fetch first page, estimate total & pages\n",
    "html = fetch(START_URL)\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "total_entries, next_href = parse_total_and_next(soup)\n",
    "page_size = 200  # per Wiktionary category page\n",
    "estimated_pages = math.ceil(total_entries / page_size) if total_entries else None\n",
    "\n",
    "all_lemmas = []\n",
    "page_idx = 0\n",
    "\n",
    "# 2) Progress bar over pages (uses estimated total if available)\n",
    "with tqdm(total=estimated_pages, unit=\"page\", desc=\"Downloading Welsh lemmas\", disable=(estimated_pages is None)) as pbar:\n",
    "    # Process the first page\n",
    "    all_lemmas.extend(parse_lemmas_on_page(soup))\n",
    "    page_idx += 1\n",
    "    if estimated_pages is not None:\n",
    "        pbar.update(1)\n",
    "\n",
    "    # 3) Follow \"next page\" links until done\n",
    "    while next_href:\n",
    "        next_url = urljoin(BASE, next_href)\n",
    "        html = fetch(next_url)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        all_lemmas.extend(parse_lemmas_on_page(soup))\n",
    "        total_entries, next_href = parse_total_and_next(soup)\n",
    "        page_idx += 1\n",
    "        if estimated_pages is not None:\n",
    "            pbar.update(1)\n",
    "\n",
    "# If total was unknown initially, print how many pages we actually walked\n",
    "print(f\"Visited {page_idx} page(s). Collected {len(all_lemmas)} raw titles.\")\n",
    "\n",
    "# 4) De-duplicate (preserve order), then sort case-insensitively for stability\n",
    "seen = set()\n",
    "deduped = []\n",
    "for t in all_lemmas:\n",
    "    if t not in seen:\n",
    "        seen.add(t)\n",
    "        deduped.append(t)\n",
    "\n",
    "deduped_sorted = sorted(deduped, key=lambda s: s.casefold())\n",
    "\n",
    "# 5) Save\n",
    "with open(\"welsh_lemmas.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(deduped_sorted, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(deduped_sorted)} unique lemmas to welsh_lemmas.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7767fd87-0512-4400-afc8-7f26ad8be51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13421 items loaded from lemmae.json\n",
      "13414\n",
      "[' ', \"'\", '-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'y', 'â', 'ê', 'î', 'ô', 'û', 'ŵ', 'ŷ'] 13414\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Path to the dictionary files\n",
    "dic_path = f\"lemmae.json\"\n",
    "\n",
    "try:\n",
    "    with open(dic_path) as f:\n",
    "        content = f.read()\n",
    "        if not content.strip():\n",
    "            raise ValueError(\"The JSON file is empty.\")\n",
    "        lemmas = json.loads(content)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {dic_path}\")\n",
    "    lemmas = []\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    lemmas = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Invalid JSON content in {dic_path}\")\n",
    "    lemmas = []\n",
    "\n",
    "print(f\"{len(lemmas)} items loaded from {dic_path}\")\n",
    "\n",
    "# Function to extract lemmas from a .dic file\n",
    "def extract_lemmas(lemmae):\n",
    "    lemmas = []\n",
    "    chars_to_filter_out = {',', '’', '.', '/', '4', ':', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z','È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ò', 'Ó', 'Ô', 'Ö', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'à', 'á', 'ã', 'ä', 'è', 'é', 'ë', 'ì', 'í', 'ï', 'ò', 'ó', 'ö', 'ù', 'ú', 'ü', 'ý', 'ÿ', 'Ŵ', 'Ŷ', 'Ÿ', '̀', '́', '̂', '̈', 'Ẁ', 'ẁ', 'Ẃ', 'ẃ', 'Ẅ', 'ẅ', 'Ẏ', 'ẏ', 'Ỳ', 'ỳ', '\\u200c', '◌', 'Ꝺ', 'ꝺ', 'Ᵹ', 'Ꝿ', 'ꝿ', 'Ꞁ', 'ꞁ', 'Ꞇ', 'ꞇ'}\n",
    "    for lemma in lemmae:\n",
    "        if not ((len(lemma) == 2 or len(lemma) == 3) and lemma[0] == \"'\") and not any(char in lemma for char in chars_to_filter_out) and lemma[-1] != \"-\" and lemma[0] != \"-\" and not any([l == lemma for l in [\"gw\", \"ch\", \"rh\", \"th\", \"dd\", \"ng\", \"ff\", \"ph\", \"ll\"]]):\n",
    "            lemmas.append(lemma)\n",
    "        else:\n",
    "            pass\n",
    "            # print(lemma)\n",
    "    print(len(lemmas))\n",
    "    return lemmas\n",
    "\n",
    "lemmas = set(extract_lemmas(lemmas))\n",
    "                           \n",
    "# Print the first 10 lemmas as a sample\n",
    "chars = list({char for string in lemmas for char in string})\n",
    "types = list(lemmas)\n",
    "chars.sort()\n",
    "types.sort()\n",
    "\n",
    "print(chars, len(types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "077c398e-218e-4e7a-83e8-4ac9d2aca13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13414 items extracted and saved to lemmae.json\n"
     ]
    }
   ],
   "source": [
    "# Dump the lemmas to a json file\n",
    "import json\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = f\"lemmae.json\"\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(types, outfile, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"{len(types)} items extracted and saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e33230a-bfd8-47b5-b230-908559b1abe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
