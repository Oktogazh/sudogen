{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f798551-a6cc-4ef9-9732-d431131695c6",
   "metadata": {},
   "source": [
    "# 2 Training the model\n",
    "Before starting, we will load a list of lemmas from the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "085e59b6-717d-42e4-aff1-665b4ff1baf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13414 items loaded from lemmae.json\n"
     ]
    }
   ],
   "source": [
    "# Dump the lemmas to a json file\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Change this variable to load another list of lemmas\n",
    "locale = \"cy_GB\"\n",
    "\n",
    "# Define the file path\n",
    "file_path = f\"lemmae.json\"\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "try:\n",
    "    with open(file_path) as f:\n",
    "        content = f.read()\n",
    "        if not content.strip():\n",
    "            raise ValueError(\"The JSON file is empty.\")\n",
    "        lemmas = json.loads(content)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    lemmas = []\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    lemmas = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Invalid JSON content in {file_path}\")\n",
    "    lemmas = []\n",
    "\n",
    "print(f\"{len(lemmas)} items loaded from {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3769fee2-3bb1-43f2-95a5-89718a207a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in /Users/alan/miniconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /Users/alan/miniconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (2025.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ensure you have the necessary library\n",
    "%pip install 'numpy<2', torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab31614-0886-4a24-9f39-b9598ceb9682",
   "metadata": {},
   "source": [
    "## 2 Defining the Model\n",
    "\n",
    "In this part we design our network. We first initialize a PyTorch [module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) by defining the different parts of the network: an embedding layer to turn each character in a 16 dimensional vector (an array of 16 numbers), one LSTM cell (`layers_number`) that will do the actual pattern recognition and prediction work and the linear fully connected (self.fc) layer converts these predictions in a simple discrete value, i.e. the index of the next character.\n",
    "\n",
    "The forward function defines the order in which the input data will go through the network. It outputs the prediction and the updated hidden layer of the LSTM cells (these hidden states are updated even during the forward pass). And finally we have a function initializing the these hidden states with empty tensors of the good shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e31fe8f5-60ae-4ea4-9bc4-eb01bc67ed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders ready:\n",
      " <torch.utils.data.dataloader.DataLoader object at 0x113194050> \n",
      " <torch.utils.data.dataloader.DataLoader object at 0x11659e4e0>\n",
      "Model ready! Total number of parameters: 75451\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, vocab, separator_tag=None):\n",
    "        self.sequences = sequences\n",
    "        self.vocab = vocab\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "        self.idx_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
    "        if separator_tag != None:\n",
    "            self.sep_tag = separator_tag\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_seq = [self.char_to_idx[char] for char in sequence[:-1]]\n",
    "        target_seq = [self.char_to_idx[char] for char in sequence[1:]]\n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)\n",
    "\n",
    "# In this case \"vocab\" is literally the latin alphabet\n",
    "vocab = sorted(set(\"\".join(lemmas)))\n",
    "dataset = CharDataset(lemmas, vocab)\n",
    "\n",
    "random.shuffle(lemmas)\n",
    "percent_len = len(lemmas)//1000\n",
    "sequences = [\"\\n\" + \"\\n\".join(lemmas[(n-1)*percent_len:n*percent_len])+ \"\\n\" for n in range(1, 1001)]\n",
    "seq_training = sequences[:85]\n",
    "seq_validating = sequences[85:]\n",
    "vocab = sorted(set(\"\".join(sequences)))\n",
    "dataset = CharDataset(seq_training, vocab, \"\\n\")\n",
    "dataset_eval = CharDataset(seq_validating, vocab, \"\\n\")\n",
    "dataloader = DataLoader(dataset, shuffle=True)\n",
    "dataloader_eval = DataLoader(dataset_eval, shuffle=True)\n",
    "print(\"Data loaders ready:\\n\", dataloader, \"\\n\", dataloader_eval)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=4, hidden_dim=16, layers_number=1, char_to_idx={}, idx_to_char={}):\n",
    "        super().__init__()\n",
    "        vocab_size = len(char_to_idx.keys())\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.idx_to_char = idx_to_char\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, layers_number, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    # The forward function is the one getting called everytime\n",
    "    # the model created by an instance of this class is called\n",
    "    # model(x, hidden) == model.forward(x, hidden)\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return (torch.zeros(layers_number, batch_size , hidden_dim),\n",
    "                torch.zeros(layers_number, batch_size , hidden_dim))\n",
    "\n",
    "# Example usage\n",
    "embedding_dim = 8\n",
    "hidden_dim = 128\n",
    "layers_number = 1\n",
    "char_to_idx = dataset.char_to_idx\n",
    "idx_to_char = dataset.idx_to_char\n",
    "\n",
    "model = LSTMModel(embedding_dim, hidden_dim, layers_number, char_to_idx, idx_to_char)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model ready! Total number of parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab4b7f-a49e-4ddb-b71e-3f0227a5076f",
   "metadata": {},
   "source": [
    "# 2.3 Training\n",
    "After defining a couple of hyperparameters, we are ready to train our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7062113b-8075-468d-af65-c2faf1a0b5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██▊                         | 1/10 [00:03<00:31,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 2.2128, Validation Loss: 2.2947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████▌                      | 2/10 [00:06<00:26,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Loss: 2.1021, Validation Loss: 2.1410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████▍                   | 3/10 [00:09<00:22,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Training Loss: 2.0318, Validation Loss: 1.9774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████▏                | 4/10 [00:12<00:18,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Training Loss: 1.9398, Validation Loss: 1.9839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████              | 5/10 [00:16<00:16,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Training Loss: 1.9249, Validation Loss: 1.9802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████▊           | 6/10 [00:19<00:12,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Training Loss: 1.8817, Validation Loss: 2.0232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████▌        | 7/10 [00:22<00:09,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Training Loss: 2.1535, Validation Loss: 2.1158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████▍     | 8/10 [00:25<00:06,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Training Loss: 1.8913, Validation Loss: 2.1524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████▏  | 9/10 [00:28<00:03,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Training Loss: 1.8522, Validation Loss: 1.6765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10/10 [00:31<00:00,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Training Loss: 1.8369, Validation Loss: 1.5490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.005\n",
    "vocab_size = len(char_to_idx)\n",
    "\n",
    "# Loss function and optimizer\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    \n",
    "    random.shuffle(lemmas)\n",
    "    percent_len = len(lemmas)//1000\n",
    "    sequences = [\"\\n\" + \"\\n\".join(lemmas[(n-1)*percent_len:n*percent_len])+ \"\\n\" for n in range(1, 1001)]\n",
    "    seq_training = sequences[:900]\n",
    "    seq_validating = sequences[900:]\n",
    "    vocab = sorted(set(\"\".join(sequences)))\n",
    "    dataset = CharDataset(seq_training, vocab, \"\\n\")\n",
    "    dataset_eval = CharDataset(seq_validating, vocab, \"\\n\")\n",
    "    dataloader = DataLoader(dataset, shuffle=True)\n",
    "    dataloader_eval = DataLoader(dataset_eval, shuffle=True)\n",
    "    \n",
    "    # first, train the model\n",
    "    model.train()\n",
    "    hidden = model.init_hidden()\n",
    "    training_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, hidden = model.forward(inputs, hidden)\n",
    "        loss = cross_entropy(outputs.view(-1, vocab_size), targets.squeeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss = loss.item()\n",
    "        hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "        \n",
    "    # second, evaluate the model to avoid overfitting\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader_eval:\n",
    "        hidden = model.init_hidden()\n",
    "\n",
    "        # forward pass\n",
    "        outputs, hidden = model.forward(inputs, hidden)\n",
    "        loss = cross_entropy(outputs.view(-1, vocab_size), targets.squeeze(0))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss_eval = total_loss / len(dataloader_eval)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss_eval:.4f}, Validation Loss: {training_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ece9b-95d1-44c8-8415-4d52e6739b8b",
   "metadata": {},
   "source": [
    "## 2.4 Generating the Pseudo-words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "750c8237-8b24-420f-bcac-594389249734",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hunspell/fr.aff'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspylls\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhunspell\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dictionary\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m dictionary = \u001b[43mDictionary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_files\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhunspell/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlocale\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_pseudoword\u001b[39m(model, length=\u001b[32m15\u001b[39m, temperature=\u001b[32m0.87\u001b[39m):\n\u001b[32m      9\u001b[39m     model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/spylls/hunspell/dictionary.py:141\u001b[39m, in \u001b[36mDictionary.from_files\u001b[39m\u001b[34m(cls, path)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.DISTRIBUTED \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(path + \u001b[33m'\u001b[39m\u001b[33m.aff\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    139\u001b[39m     path = os.path.join(os.path.dirname(os.path.realpath(\u001b[34m__file__\u001b[39m)), \u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mcls\u001b[39m.DISTRIBUTED[path], path)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m aff, context = readers.read_aff(\u001b[43mFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.aff\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    142\u001b[39m dic = readers.read_dic(FileReader(path + \u001b[33m'\u001b[39m\u001b[33m.dic\u001b[39m\u001b[33m'\u001b[39m, encoding=context.encoding), aff=aff, context=context)\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(aff, dic)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/spylls/hunspell/readers/file_reader.py:67\u001b[39m, in \u001b[36mFileReader.__init__\u001b[39m\u001b[34m(self, path, encoding)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, encoding=\u001b[33m'\u001b[39m\u001b[33mWindows-1252\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     66\u001b[39m     \u001b[38;5;28mself\u001b[39m.path = path\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/spylls/hunspell/readers/file_reader.py:75\u001b[39m, in \u001b[36mFileReader._open\u001b[39m\u001b[34m(self, path, encoding)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, encoding):  \u001b[38;5;66;03m# pylint: disable=no-self-use\u001b[39;00m\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# errors='surrogateescape', because at least hu_HU dictionary of LibreOffice uses invalid\u001b[39;00m\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# in UTF-8 single-bytes as suffix flags\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msurrogateescape\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'hunspell/fr.aff'"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "import torch.nn.functional as F\n",
    "from spylls.hunspell import Dictionary\n",
    "import sys\n",
    "dictionary = Dictionary.from_files(f\"hunspell/{locale}\")\n",
    "\n",
    "\n",
    "def generate_pseudoword(model, length=15, temperature=0.87):\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(1)\n",
    "    start_seq = [0]\n",
    "    inputs = torch.tensor(start_seq).unsqueeze(0)  # Shape: (1, seq_len)\n",
    "    generated_seq = []\n",
    "    words_generated = set([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while len(words_generated) < length:\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "\n",
    "            # outputs shape: (1, seq_len, vocab_size)\n",
    "            # We need the last time step's output for the next prediction\n",
    "            last_output = outputs[:, -1]  # Shape: (1, vocab_size)\n",
    "\n",
    "            # Apply temperature scaling\n",
    "            last_output = last_output / temperature\n",
    "            probs = F.softmax(last_output, dim=-1).squeeze(0)  # the multinomial accepts only one order tensors\n",
    "\n",
    "            # Ensure all the probabilities are valid\n",
    "            if torch.isnan(probs).any() or torch.isinf(probs).any() or (probs < 0).any():\n",
    "                print(\"Invalid probabilities detected. Resetting to uniform distribution.\")\n",
    "                probs = torch.ones_like(probs) / probs.size(0)\n",
    "\n",
    "            # Sample the next character\n",
    "            predicted_idx = torch.multinomial(probs, 1).item()\n",
    "            generated_seq.append(predicted_idx)\n",
    "            inputs = torch.tensor([[predicted_idx]])  # Shape: (1, 1)\n",
    "\n",
    "            if vocab[predicted_idx] == \"\\n\":\n",
    "                new_word = ''.join([vocab[i] for i in generated_seq[:-1]])\n",
    "                generated_seq = []\n",
    "                if not dictionary.lookup(new_word.capitalize()) and new_word not in lemmas:\n",
    "                    words_generated.add(new_word)\n",
    "                sys.stdout.write(f\"\\r{len(words_generated)} words so far\")\n",
    "\n",
    "    return list(sorted(words_generated))\n",
    "\n",
    "# Example usage\n",
    "generated_pseudoword = generate_pseudoword(model, 1000)\n",
    "print()\n",
    "print(\"\\n\".join(generated_pseudoword[:10]))\n",
    "print(\"\\n\".join(generated_pseudoword[-40:]))\n",
    "print(len(\"\\n\".join(generated_pseudoword))/len(generated_pseudoword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172bd61d-2857-4b28-a8bb-115e7f17508a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
