{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78b8d16a-94e7-4448-aafe-20e7181d8fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Frequency Words Scraper (Belgium, finance)\n",
      "==================================================\n",
      "[1/6] Fetching 1-1000: https://en.wiktionary.org/wiki/Wiktionary:French_frequency_lists_(Belgium,_finance)/1-1000\n",
      "  → Found 919 words\n",
      "[2/6] Fetching 1001-2000: https://en.wiktionary.org/wiki/Wiktionary:French_frequency_lists_(Belgium,_finance)/1001-2000\n",
      "  → Found 866 words\n",
      "[3/6] Fetching 2001-4000: https://en.wiktionary.org/wiki/Wiktionary:French_frequency_lists_(Belgium,_finance)/2001-4000\n",
      "  → Found 1693 words\n",
      "[4/6] Fetching 4001-6000: https://en.wiktionary.org/wiki/Wiktionary:French_frequency_lists_(Belgium,_finance)/4001-6000\n",
      "  → Found 1667 words\n",
      "[5/6] Fetching 6001-8000: https://en.wiktionary.org/wiki/Wiktionary:French_frequency_lists_(Belgium,_finance)/6001-8000\n",
      "  → Found 1629 words\n",
      "[6/6] Fetching 8001-10000: https://en.wiktionary.org/wiki/Wiktionary:French_frequency_lists_(Belgium,_finance)/8001-10000\n",
      "  → Found 1549 words\n",
      "\n",
      "Summary\n",
      "--------------------------------------------------\n",
      "       1-1000:     0 words\n",
      "    1001-2000:     0 words\n",
      "    2001-4000:     0 words\n",
      "    4001-6000:     0 words\n",
      "    6001-8000:     0 words\n",
      "   8001-10000:     0 words\n",
      "\n",
      "Data saved to fr-frequencies.json\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from typing import Dict, List\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "\n",
    "BASE_URL = \"https://en.wiktionary.org/wiki/Wiktionary:French_frequency_lists_(Belgium,_finance)/\"\n",
    "RANGES = [\"1-1000\", \"1001-2000\", \"2001-4000\", \"4001-6000\", \"6001-8000\", \"8001-10000\"]\n",
    "OUTPUT_JSON = \"fr-frequencies.json\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/124.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "def make_session() -> requests.Session:\n",
    "    \"\"\"Create a requests session with polite retry/backoff.\"\"\"\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=(429, 500, 502, 503, 504),\n",
    "        allowed_methods=frozenset([\"GET\"]),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.headers.update(HEADERS)\n",
    "    return session\n",
    "\n",
    "\n",
    "def page_url_for_range(range_label: str) -> str:\n",
    "    \"\"\"Construct the subpage URL like .../(1-1000).\"\"\"\n",
    "    return urljoin(BASE_URL, f\"{range_label}\")\n",
    "\n",
    "\n",
    "def fetch_html(session: requests.Session, url: str) -> BeautifulSoup:\n",
    "    \"\"\"Fetch and parse HTML for a URL.\"\"\"\n",
    "    resp = session.get(url, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    return BeautifulSoup(resp.content, \"html.parser\")\n",
    "\n",
    "\n",
    "def extract_words_from_page(soup: BeautifulSoup) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract French words from the page.\n",
    "    The page layout uses a <table> containing <ul><li> entries like:\n",
    "      8001. <span class=\"Latn\" lang=\"fr\"><a href=\"/wiki/obtenue#French\">obtenue</a></span>\n",
    "    Strategy:\n",
    "      - Look inside tables (to stay scoped to the frequency list)\n",
    "      - Find <a> with href containing '#French'\n",
    "      - Ensure the anchor is within an element marked lang=\"fr\"\n",
    "    \"\"\"\n",
    "    words: List[str] = []\n",
    "\n",
    "    # Restrict to tables to avoid side links / navigation\n",
    "    tables = soup.find_all(\"table\")\n",
    "    for table in tables:\n",
    "        # Find anchors that link to the French section\n",
    "        for a in table.select('a[href*=\"#French\"]'):\n",
    "            # Ensure we are in a French-language span/container\n",
    "            if a.find_parent(attrs={\"lang\": \"fr\"}) is None:\n",
    "                continue\n",
    "            text = a.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "            words.append(text)\n",
    "\n",
    "    # De-duplicate while preserving order\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for w in words:\n",
    "        if w not in seen:\n",
    "            seen.add(w)\n",
    "            deduped.append(w)\n",
    "    return deduped\n",
    "\n",
    "\n",
    "def fetch_french_frequency_words() -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Fetch French frequency words from the five (actually six) subpages.\n",
    "    Returns a dict keyed by the full range string, e.g. \"1-1000\".\n",
    "    \"\"\"\n",
    "    session = make_session()\n",
    "    result: Dict[str, List[str]] = {}\n",
    "\n",
    "    print(\"French Frequency Words Scraper (Belgium, finance)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for i, range_label in enumerate(RANGES, 1):\n",
    "        url = page_url_for_range(range_label)\n",
    "        print(f\"[{i}/{len(RANGES)}] Fetching {range_label}: {url}\")\n",
    "        try:\n",
    "            soup = fetch_html(session, url)\n",
    "            words = extract_words_from_page(soup)\n",
    "            print(f\"  → Found {len(words)} words\")\n",
    "            result[range_label.split(\"-\")[-1]] = words\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"  ! HTTP error for {range_label}: {e}\")\n",
    "            result[range_label] = []\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"  ! Request error for {range_label}: {e}\")\n",
    "            result[range_label] = []\n",
    "        except Exception as e:\n",
    "            print(f\"  ! Parsing error for {range_label}: {e}\")\n",
    "            result[range_label] = []\n",
    "\n",
    "        # Be polite to the server\n",
    "        time.sleep(0.6)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def save_to_json(data: Dict[str, List[str]], filename: str = OUTPUT_JSON) -> bool:\n",
    "    \"\"\"Save the frequency dictionary to a JSON file (UTF-8, pretty).\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\nData saved to {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving to JSON: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = fetch_french_frequency_words()\n",
    "\n",
    "    nonempty = {k: v for k, v in data.items() if v}\n",
    "    print(\"\\nSummary\")\n",
    "    print(\"-\" * 50)\n",
    "    for rng in RANGES:\n",
    "        count = len(data.get(rng, []))\n",
    "        print(f\"  {rng:>11}: {count:>5} words\")\n",
    "\n",
    "    if nonempty:\n",
    "        save_to_json(data, OUTPUT_JSON)\n",
    "        print(\"\\nDone.\")\n",
    "    else:\n",
    "        print(\"\\nNo data extracted. Check the site structure or selectors.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56a1ec99-cdfa-4928-8e1d-945b2b4165f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'w': 'a b c', 'r': 200}, {'w': 'a capella', 'r': 200}, {'w': 'a cappella', 'r': 200}, {'w': 'a fortiori', 'r': 200}, {'w': 'a giorno', 'r': 200}, {'w': 'a posteriori', 'r': 200}, {'w': 'a priori', 'r': 200}, {'w': 'ab intestat', 'r': 200}, {'w': 'abaca', 'r': 200}, {'w': 'abaissable', 'r': 200}]\n",
      "dict_keys(['1000', '2000', '4000', '6000', '8000', '10000'])\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# Define the file path\n",
    "file_path = f\"items.json\"\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "try:\n",
    "    with open(file_path) as f:\n",
    "        content = f.read()\n",
    "        if not content.strip():\n",
    "            raise ValueError(\"The JSON file is empty.\")\n",
    "        items = json.loads(content)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    items = []\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    items = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Invalid JSON content in {file_path}\")\n",
    "    items = []\n",
    "\n",
    "# Define the file path\n",
    "freq = f\"fr-frequencies.json\"\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "try:\n",
    "    with open(freq) as f:\n",
    "        content = f.read()\n",
    "        if not content.strip():\n",
    "            raise ValueError(\"The JSON file is empty.\")\n",
    "        frequencies = json.loads(content)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    items = []\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    items = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Invalid JSON content in {file_path}\")\n",
    "    items = []\n",
    "\n",
    "print(items[\"keys\"][:10])\n",
    "print(frequencies.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ec9cbe-c811-4fab-a6ae-0e89afd83e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|█| 45270/45270 [00:03<00:00, 12670.96it/s\n",
      "Processing items: 100%|█| 50000/50000 [00:00<00:00, 751473.45it/\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from math import floor\n",
    "\n",
    "new_keys = []\n",
    "for item in tqdm(items[\"keys\"], desc=\"Processing items\"):\n",
    "    if item[\"w\"] in frequencies[\"1000\"]:\n",
    "        val = random.randint(1, 400)\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    elif item[\"w\"] in frequencies[\"2000\"]:\n",
    "        val = random.randint(400, 500)\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    elif item[\"w\"] in frequencies[\"4000\"]:\n",
    "        val = random.randint(500, 700)\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    elif item[\"w\"] in frequencies[\"6000\"]:\n",
    "        val = random.randint(700, 800)\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    elif item[\"w\"] in frequencies[\"8000\"]:\n",
    "        val = random.randint(800, 875)\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    elif item[\"w\"] in frequencies[\"10000\"]:\n",
    "        val = random.randint(875, 950)\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    else:\n",
    "        val = floor(random.randint(950, 2000))\n",
    "        val -= val%5\n",
    "        new_keys.append(\n",
    "            {\"w\": item[\"w\"], \"r\": val}\n",
    "        )\n",
    "    \n",
    "\n",
    "# order the items by rating\n",
    "new_keys.sort(key=lambda x: x[\"r\"])\n",
    "items[\"keys\"] = new_keys\n",
    "\n",
    "\n",
    "new_dis = []\n",
    "for item in tqdm(items[\"distractors\"], desc=\"Processing items\"):\n",
    "    val = floor(random.randint(0, 2000))\n",
    "    val -= val%5\n",
    "    new_dis.append(\n",
    "        {\"w\": item[\"w\"], \"r\": val}\n",
    "    )\n",
    "new_dis.sort(key=lambda x: x[\"r\"])\n",
    "items[\"distractors\"] = new_dis\n",
    "\n",
    "# Write the updated items back to the JSON file\n",
    "with open(file_path, \"w\") as f:\n",
    "    json.dump(items, f, indent=0, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92c5b4-781b-4819-b4dc-b1377d6fec0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
