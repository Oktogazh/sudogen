{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ddbb4cb-fa29-41cb-b2cd-3faf43ad832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/alan/miniconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: spylls in /Users/alan/miniconda3/lib/python3.12/site-packages (0.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alan/miniconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alan/miniconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alan/miniconda3/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alan/miniconda3/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests spylls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a3eca8-7003-4ccc-aa6a-5d2d77b1250c",
   "metadata": {},
   "source": [
    "## Download a Hunspell Dictionary\n",
    "Clicking on [this link](https://mozilla-l10n.github.io/firefox-dictionaries/complete.html) you will find a list of available and up-to-date dictionaries.\n",
    "Find the dictionary you want to train your model on. Once on the page of the dictionary you want to download, instead of clicking on \"add to Firefox\", right-click and select \"copy the link\". Then past the value to assign it to the variable `dictionary_url`.\n",
    "Think to also set the value of the variable `locale`, checkout the column \"Dictionary Locale\" of the table in the list of the dictionaries, as they might not be shaped the same way, for example, Welsh is \"cy_GB\" but British English is \"en-GB\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02993c52-c529-4f9d-a069-ffcd248a8198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr dictionary decompressed\n",
      "Folder cleaned successfully.\n",
      "Check out your dictionary in ./hunspell\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Replace this link\n",
    "# for Breton (be): https://addons.mozilla.org/firefox/downloads/file/4270474/difazier_an_drouizig-0.17resigned1.xpi\n",
    "# for Welsh (cy_GB): https://addons.mozilla.org/firefox/downloads/file/4270302/geiriadur_cymraeg-1.8.3resigned1.xpi\n",
    "# for English (en-GB): https://addons.mozilla.org/firefox/downloads/file/4270302/geiriadur_cymraeg-1.8.3resigned1.xpi\n",
    "# for Dutch (nl): https://addons.mozilla.org/firefox/downloads/file/3776797/woordenboek_nederlands-4.20.19.xpi\n",
    "dictionary_url = \"https://addons.mozilla.org/firefox/downloads/file/3581786/dictionnaire_francais1-7.0b.xpi\"\n",
    "\n",
    "# Replace with the appropriate ISO-369 code\n",
    "locale = \"fr\"\n",
    "\n",
    "if os.path.isdir(f\"./{locale}\"):\n",
    "    shutil.rmtree(f\"./{locale}\")\n",
    "\n",
    "# Download and extract dictionary\n",
    "response = requests.get(dictionary_url)\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    z.extractall(\"locales\")\n",
    "    print(f\"{locale} dictionary decompressed\")\n",
    "\n",
    "# standardize name of files\n",
    "for file in os.listdir(f\"./\"):\n",
    "    if file.endswith(\".dic\"):\n",
    "        os.rename(f\"./dictionaries/{file}\", f\"./{locale}.dic\")\n",
    "    elif file.endswith(\".aff\"):\n",
    "        os.rename(f\"./dictionaries/{file}\", f\"./{locale}.aff\")\n",
    "\n",
    "try:\n",
    "    files = os.listdir(\"locales\")\n",
    "    shutil.rmtree(\"locales/META-INF/\")\n",
    "    for file in files:\n",
    "        file_path = os.path.join(\"locales\", file)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "    print(\"Folder cleaned successfully.\")\n",
    "except OSError:\n",
    "    print(\"Error occurred while deleting files.\")\n",
    "\n",
    "os.rename(\"locales/dictionaries\", f\"./hunspell\")\n",
    "os.rmdir(\"locales\")\n",
    "print(\"Check out your dictionary in\", f\"./hunspell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d90365-7ed3-47a7-afb9-dfe426e63095",
   "metadata": {},
   "source": [
    "Extract and filter the types from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34e26258-77bb-461b-bf78-ddff4b7a6fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Dictionary Scraper - Usito\n",
      "========================================\n",
      "Processing 27 letter sections...\n",
      "Fetching words starting with 'a'...\n",
      "  Found 3717 valid words for letter 'a'\n",
      "Fetching words starting with 'b'...\n",
      "  Found 2447 valid words for letter 'b'\n",
      "Fetching words starting with 'c'...\n",
      "  Found 5332 valid words for letter 'c'\n",
      "Fetching words starting with 'd'...\n",
      "  Found 3053 valid words for letter 'd'\n",
      "Fetching words starting with 'e'...\n",
      "  Found 3024 valid words for letter 'e'\n",
      "Fetching words starting with 'f'...\n",
      "  Found 1742 valid words for letter 'f'\n",
      "Fetching words starting with 'g'...\n",
      "  Found 1541 valid words for letter 'g'\n",
      "Fetching words starting with 'h'...\n",
      "  Found 1193 valid words for letter 'h'\n",
      "Fetching words starting with 'i'...\n",
      "  Found 2015 valid words for letter 'i'\n",
      "Fetching words starting with 'j'...\n",
      "  Found 395 valid words for letter 'j'\n",
      "Fetching words starting with 'k'...\n",
      "  Found 211 valid words for letter 'k'\n",
      "Fetching words starting with 'l'...\n",
      "  Found 1284 valid words for letter 'l'\n",
      "Fetching words starting with 'm'...\n",
      "  Found 2784 valid words for letter 'm'\n",
      "Fetching words starting with 'n'...\n",
      "  Found 857 valid words for letter 'n'\n",
      "Fetching words starting with 'o'...\n",
      "  Found 1017 valid words for letter 'o'\n",
      "Fetching words starting with 'p'...\n",
      "  Found 4523 valid words for letter 'p'\n",
      "Fetching words starting with 'q'...\n",
      "  Found 235 valid words for letter 'q'\n",
      "Fetching words starting with 'r'...\n",
      "  Found 2642 valid words for letter 'r'\n",
      "Fetching words starting with 's'...\n",
      "  Found 3313 valid words for letter 's'\n",
      "Fetching words starting with 't'...\n",
      "  Found 2328 valid words for letter 't'\n",
      "Fetching words starting with 'u'...\n",
      "  Found 225 valid words for letter 'u'\n",
      "Fetching words starting with 'v'...\n",
      "  Found 1151 valid words for letter 'v'\n",
      "Fetching words starting with 'w'...\n",
      "  Found 73 valid words for letter 'w'\n",
      "Fetching words starting with 'x'...\n",
      "  Found 25 valid words for letter 'x'\n",
      "Fetching words starting with 'y'...\n",
      "  Found 49 valid words for letter 'y'\n",
      "Fetching words starting with 'z'...\n",
      "  Found 125 valid words for letter 'z'\n",
      "Fetching words starting with 'Autres'...\n",
      "  Found 8 valid words for letter 'Autres'\n",
      "\n",
      "Completed! Found 45309 unique French dictionary entries\n",
      "Sample words: ['abaca', 'abaissable', 'abaissant', 'abaisse', 'abaisse-langue', 'abaissement', 'abaisser', 'abaisseur', 'abajoue', 'abandogiciel']\n",
      "\n",
      "First 20 words: ['abaca', 'abaissable', 'abaissant', 'abaisse', 'abaisse-langue', 'abaissement', 'abaisser', 'abaisseur', 'abajoue', 'abandogiciel', 'abandon', 'abandongiciel', 'abandonner', 'abandonware', 'abaque', 'abasourdir', 'abasourdissant', 'abasourdissement', 'abat', 'abatage']\n",
      "Last 20 words: ['zostérien', 'zouave', 'zoulou', 'zozo', 'zozotement', 'zozoter', 'zucchini', 'zumba', 'zut', 'zyeuter', 'zygomatique', 'zygote', \"''\", 'µm', 'µs', '2n', '3n', '4n', '4 x 4', '911']\n",
      "No existing file found, starting fresh\n",
      "Combined: 0 existing + 45309 new = 45309 unique entries\n",
      "Lemmae saved to lemmae.json\n",
      "\n",
      "Script completed successfully!\n",
      "Variable 'lemmae' contains 45309 French dictionary entries\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "\n",
    "def fetch_words_from_letter_page(base_url, letter):\n",
    "    \"\"\"\n",
    "    Fetch all words from a specific letter page.\n",
    "    Returns a list of words that meet the criteria (more than one letter, no uppercase).\n",
    "    \"\"\"\n",
    "    url = f\"{base_url}/{letter}\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"Fetching words starting with '{letter}'...\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        words = []\n",
    "        \n",
    "        # Find the ul element with class \"entrées\"\n",
    "        entries_ul = soup.find('ul', class_='entrées')\n",
    "        \n",
    "        if entries_ul:\n",
    "            # Find all li elements within the entries list\n",
    "            list_items = entries_ul.find_all('li')\n",
    "            \n",
    "            for li in list_items:\n",
    "                # Find the link within each li\n",
    "                link = li.find('a')\n",
    "                if link:\n",
    "                    word = link.get_text().strip()\n",
    "                    \n",
    "                    # Apply filters:\n",
    "                    # 1. More than one letter\n",
    "                    # 2. No uppercase letters\n",
    "                    # 3. Remove entries with special characters that aren't words\n",
    "                    if (len(word) > 1 and \n",
    "                        not any(c.isupper() for c in word) and\n",
    "                        not (word.endswith('-') or word.startswith('-'))):  # Additional check for all caps\n",
    "                        \n",
    "                        words.append(word)\n",
    "            \n",
    "            print(f\"  Found {len(words)} valid words for letter '{letter}'\")\n",
    "        else:\n",
    "            print(f\"  No entries found for letter '{letter}'\")\n",
    "        \n",
    "        return words\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching page for letter '{letter}': {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing page for letter '{letter}': {e}\")\n",
    "        return []\n",
    "\n",
    "def fetch_all_french_dictionary_words():\n",
    "    \"\"\"\n",
    "    Fetch all French dictionary words from the Usito dictionary.\n",
    "    Returns a list of words that meet the criteria.\n",
    "    \"\"\"\n",
    "    base_url = \"https://usito.usherbrooke.ca/index/mots/tous\"\n",
    "    lemmae = []\n",
    "    \n",
    "    # Letters to process (including 'Autres' for other characters)\n",
    "    letters = list(string.ascii_lowercase) + ['Autres']\n",
    "    \n",
    "    print(\"French Dictionary Scraper - Usito\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Processing {len(letters)} letter sections...\")\n",
    "    \n",
    "    for letter in letters:\n",
    "        try:\n",
    "            words = fetch_words_from_letter_page(base_url, letter)\n",
    "            lemmae.extend(words)\n",
    "            \n",
    "            # Be respectful to the server\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\nScraping interrupted by user at letter '{letter}'\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error processing letter '{letter}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_lemmae = []\n",
    "    for word in lemmae:\n",
    "        if word not in seen:\n",
    "            seen.add(word)\n",
    "            unique_lemmae.append(word)\n",
    "    \n",
    "    print(f\"\\nCompleted! Found {len(unique_lemmae)} unique French dictionary entries\")\n",
    "    print(f\"Sample words: {unique_lemmae[:10] if unique_lemmae else 'None'}\")\n",
    "    \n",
    "    return unique_lemmae\n",
    "\n",
    "def save_lemmae_as_python_list(lemmae, filename=\"lemmae.json\"):\n",
    "    \"\"\"\n",
    "    Save the lemmae as a JSON list, merging with existing entries if file exists.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    existing_lemmae = []\n",
    "    \n",
    "    # Try to load existing lemmae from the file\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            existing_lemmae = json.load(f)\n",
    "        print(f\"Loaded {len(existing_lemmae)} existing entries from {filename}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No existing file found, starting fresh\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error reading JSON from {filename}, starting fresh\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading existing file: {e}, starting fresh\")\n",
    "    \n",
    "    # Combine existing and new lemmae\n",
    "    combined_lemmae = existing_lemmae + lemmae\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    unique_lemmae = []\n",
    "    for word in combined_lemmae:\n",
    "        if word not in seen:\n",
    "            seen.add(word)\n",
    "            unique_lemmae.append(word)\n",
    "    \n",
    "    print(f\"Combined: {len(existing_lemmae)} existing + {len(lemmae)} new = {len(unique_lemmae)} unique entries\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(unique_lemmae, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Lemmae saved to {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving lemmae to JSON: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the French dictionary scraper.\n",
    "    \"\"\"\n",
    "    # Fetch all words\n",
    "    lemmae = fetch_all_french_dictionary_words()\n",
    "    \n",
    "    if lemmae:\n",
    "        print(f\"\\nFirst 20 words: {lemmae[:20]}\")\n",
    "        print(f\"Last 20 words: {lemmae[-20:]}\")\n",
    "        \n",
    "        # Save in different formats\n",
    "        save_lemmae_as_python_list(lemmae)\n",
    "        \n",
    "        print(\"\\nScript completed successfully!\")\n",
    "        print(f\"Variable 'lemmae' contains {len(lemmae)} French dictionary entries\")\n",
    "    else:\n",
    "        print(\"No words were extracted. Please check the website structure.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "301c318f-ac6c-43d9-bee7-6726703c5bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45270 items loaded from lemmae.json\n",
      "45270\n",
      "{'à', 'y', 'z', 's', 'v', \"'\", 'n', 'b', '-', 'm', 'ô', 'r', 'î', 'é', 'd', 'p', 'ù', 'h', 'g', 'â', 'a', 'x', 'u', ' ', 'ö', 'q', 'ü', 'æ', 'o', 'e', 'è', 'l', 'c', 'w', 'ç', 't', 'œ', 'k', 'ï', 'û', 'j', 'ê', 'f', 'i', 'ë', 'ä'} 45270\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Path to the dictionary files\n",
    "dic_path = f\"lemmae.json\"\n",
    "\n",
    "try:\n",
    "    with open(dic_path) as f:\n",
    "        content = f.read()\n",
    "        if not content.strip():\n",
    "            raise ValueError(\"The JSON file is empty.\")\n",
    "        lemmas = json.loads(content)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {dic_path}\")\n",
    "    lemmas = []\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    lemmas = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Invalid JSON content in {dic_path}\")\n",
    "    lemmas = []\n",
    "\n",
    "print(f\"{len(lemmas)} items loaded from {dic_path}\")\n",
    "\n",
    "# Function to extract lemmas from a .dic file\n",
    "def extract_lemmas(lemmae):\n",
    "    lemmas = []\n",
    "    chars_to_filter_out = {'²', ':', '.', 'µ', '2', '/', '(', '3',  '’', '4', ',', '1', '9','8','³', 'ñ', ')'}\n",
    "    for lemma in lemmae:\n",
    "        lemma = lemma.replace(\"(-)\", \"-\")\n",
    "        lemma = lemma.replace(\"(s)\", \"s\")\n",
    "        if not any(char in lemma for char in chars_to_filter_out) and lemma[-1] != \"-\" and lemma[0] != \"-\" and lemma != \"''\":\n",
    "            lemmas.append(lemma)\n",
    "        else:\n",
    "            print(lemma)\n",
    "    print(len(lemmas))\n",
    "    return lemmas\n",
    "\n",
    "lemmas = set(extract_lemmas(lemmas))\n",
    "                           \n",
    "# Print the first 10 lemmas as a sample\n",
    "chars = {char for string in lemmas for char in string}\n",
    "types = list(lemmas)\n",
    "\n",
    "types.sort()\n",
    "\n",
    "print(chars, len(types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "beaa3a9e-030c-43cd-b2fe-b357c6e0a454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45270 items extracted and saved to lemmae.json\n"
     ]
    }
   ],
   "source": [
    "# Dump the lemmas to a json file\n",
    "import json\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = f\"lemmae.json\"\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(types, outfile, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"{len(types)} items extracted and saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1792fbb-d2b2-46eb-9b90-c005dd08581a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
