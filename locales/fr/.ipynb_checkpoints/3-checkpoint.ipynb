{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07bd40ed-7802-4d37-9a16-7d823819c2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in /Users/alan/miniconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /Users/alan/miniconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/alan/miniconda3/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: seaborn in /Users/alan/miniconda3/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /Users/alan/miniconda3/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: pandas in /Users/alan/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: filelock in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (2025.3.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/alan/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/alan/miniconda3/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/alan/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/alan/miniconda3/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/alan/miniconda3/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/alan/miniconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/alan/miniconda3/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/alan/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/alan/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install 'numpy<2', torch scikit-learn seaborn matplotlib pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5e002a0-9a48-47cb-9953-ff445287f2a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lstm_model-uk-UA.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 1: load the model from the previous chapter\u001b[39;00m\n\u001b[32m     25\u001b[39m locale = \u001b[33m\"\u001b[39m\u001b[33muk-UA\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# Change this variable if you want to inspect another language you trained\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlstm_model-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlocale\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m torch.set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 2: extract the embedding layer and its associated characters\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:998\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m    995\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m    996\u001b[39m     pickle_load_args[\u001b[33m'\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m998\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m    999\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1000\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1001\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1002\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1003\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:445\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    446\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    447\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:426\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'lstm_model-uk-UA.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 0: redeclare the LSTMModel class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=4, hidden_dim=4, layers_number=1, char_to_idx={}, idx_to_char={}):\n",
    "        super().__init__()\n",
    "        vocab_size = len(char_to_idx.keys())\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.idx_to_char = idx_to_char\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, layers_number, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    # The forward function is the one getting called everytime\n",
    "    # the model created by an instance of this class is called\n",
    "    # model(x, hidden) == model.forward(x, hidden)\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "# 1: load the model from the previous chapter\n",
    "locale = \"uk-UA\" # Change this variable if you want to inspect another language you trained\n",
    "model = torch.load(f'lstm_model-{locale}.pth')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# 2: extract the embedding layer and its associated characters\n",
    "chars = [n if n != \"\\n\" else \"\\\\n\" for n in model.idx_to_char.values()]\n",
    "embedding = model.embedding.weight.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d6dc618-b11e-476e-9327-4777b974532f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecomposition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 3 use scikit learn to do TSNE dimensionality reduction\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m X = np.array(\u001b[43membedding\u001b[49m)\n\u001b[32m     10\u001b[39m reduced_embedding = PCA(n_components=\u001b[32m2\u001b[39m).fit_transform(TSNE(n_components=\u001b[32m2\u001b[39m, perplexity=\u001b[32m6\u001b[39m).fit_transform(X))\n\u001b[32m     12\u001b[39m df = pd.DataFrame(reduced_embedding, columns=[\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'embedding' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 3 use scikit learn to do TSNE dimensionality reduction\n",
    "X = np.array(embedding)\n",
    "reduced_embedding = PCA(n_components=2).fit_transform(TSNE(n_components=2, perplexity=6).fit_transform(X))\n",
    "\n",
    "df = pd.DataFrame(reduced_embedding, columns=[\"x\", \"y\"])\n",
    "\n",
    "df[\"chars\"] = chars\n",
    "df[\"Category\"] = [\"vowel\" if c in \"аеиіоу\" else \"diphtong\" if c in \"їєюя\" else \"semi-vowel\" if c in \"йь\" else \"else\" if c in \"\\\\n'-\" else \"consonant\" for c in chars]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=\"x\", y=\"y\", data=df, hue=\"Category\")\n",
    "\n",
    "for i in range(len(chars)):\n",
    "    plt.annotate(df[\"chars\"][i], (df[\"x\"][i]+1, df[\"y\"][i]+1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e7260e-be16-40f8-97e5-b97fbd22791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "locale = \"fr\"\n",
    "lemmas_file_path = f\"lemmae.json\"\n",
    "pseudo_lemmas_file_path =  f\"pseudo-lemmae.json\"\n",
    "\n",
    "try:\n",
    "    with open(lemmas_file_path) as f:\n",
    "        content = f.read()\n",
    "        if not content.strip():\n",
    "            raise ValueError(\"The JSON file is empty.\")\n",
    "        lemmas = json.loads(content)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {lemmas_file_path}\")\n",
    "    lemmas = []\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    lemmas = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Invalid JSON content in {lemmas_file_path}\")\n",
    "\n",
    "try:\n",
    "    with open(pseudo_lemmas_file_path) as f2:\n",
    "        content2 = f2.read()\n",
    "        if not content2.strip():\n",
    "            raise ValueError(\"The JSON file is empty.\")\n",
    "        pseudo_lemmas = json.loads(content2)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {pseudo_lemmas_file_path}\")\n",
    "    pseudo_lemmas = []\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    pseudo_lemmas = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Invalid JSON content in {file_path}\")\n",
    "\n",
    "print(f\"{len(lemmas)} items loaded from {lemmas_file_path}:\\n\" + \"\\n\".join(lemmas[:10]))\n",
    "print(f\"{len(pseudo_lemmas)} items loaded from {pseudo_lemmas_file_path}:\\n\" + \"\\n\".join(pseudo_lemmas[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d4c338-0e4b-47c5-b696-575217e5f919",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m lemmas_str = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[43mlemmas\u001b[49m)\n\u001b[32m      2\u001b[39m pseudo_lemmas_str = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(pseudo_lemmas)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lemmas_str)/\u001b[38;5;28mlen\u001b[39m(lemmas))\n",
      "\u001b[31mNameError\u001b[39m: name 'lemmas' is not defined"
     ]
    }
   ],
   "source": [
    "lemmas_str = \"\".join(lemmas)\n",
    "pseudo_lemmas_str = \"\".join(pseudo_lemmas)\n",
    "print(len(lemmas_str)/len(lemmas))\n",
    "print(len(pseudo_lemmas_str)/len(pseudo_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae91696-12fa-4b08-8e6c-27fa3677940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "cmap = plt.cm.viridis\n",
    "sns.set_theme(context='notebook', style=\"whitegrid\", palette=\"pastel\")\n",
    "\n",
    "lemmas_counter = dict(Counter([len(l) for l in lemmas]))\n",
    "pseudo_lemmas_counter = dict(Counter([len(l) for l in pseudo_lemmas]))\n",
    "data = []\n",
    "for i in range(max([i for i in lemmas_counter.keys()] + [i for i in pseudo_lemmas_counter.keys()]) + 1):\n",
    "    data.append({\n",
    "        \"Lemmas Lengths\": lemmas_counter[i]/len(lemmas) if i in lemmas_counter else 0,\n",
    "        \"Pseudo-Lemmas Lengths\": pseudo_lemmas_counter[i]/len(pseudo_lemmas) if i in pseudo_lemmas_counter else 0\n",
    "    })\n",
    "\n",
    "\n",
    "lengths_df = pd.DataFrame(data)\n",
    "print(lengths_df)\n",
    "lengths_df[\"Length Difference\"] = (lengths_df[\"Pseudo-Lemmas Lengths\"] - lengths_df[\"Lemmas Lengths\"])\n",
    "\n",
    "lengths_df.index.name = 'Lengths'\n",
    "lengths_df = lengths_df.drop([0, 1])\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "\n",
    "bar_plot = sns.barplot(\n",
    "    x='Lengths',\n",
    "    y=\"Length Difference\",\n",
    "    hue=\"Length Difference\",\n",
    "    data=lengths_df\n",
    ")\n",
    "plt.yscale('symlog')\n",
    "\n",
    "# plt.title('Comparison of the Lengths of Types and Pseudo-Words', fontsize=16)\n",
    "plt.xlabel('Lengths', fontsize=12)\n",
    "plt.ylabel('Difference Divided by the Count of Items on a Logarithmic Scale', fontsize=12)\n",
    "plt.legend(title='Variable')\n",
    "plt.show()\n",
    "\n",
    "print(max(pseudo_lemmas, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0385a6-6734-4b95-8964-223965167a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.viridis\n",
    "sns.set_theme(context='notebook', style=\"whitegrid\", palette=\"pastel\")\n",
    "\n",
    "chars = set(lemmas_str + pseudo_lemmas_str)\n",
    "counter_lem = Counter(lemmas_str)\n",
    "counter_psd = Counter(pseudo_lemmas_str)\n",
    "\n",
    "data = []\n",
    "for char in chars:\n",
    "        freq_lemmas = (counter_lem.get(char, 0) / len(lemmas_str)) * 100 if lemmas_str else 0\n",
    "        freq_pseudo_lemmas = (counter_psd.get(char, 0) / len(pseudo_lemmas_str)) * 100 if pseudo_lemmas_str else 0\n",
    "        data.append({'Character': char, \"lemmas\": freq_lemmas, \"pseudo_lemmas\": freq_pseudo_lemmas})\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['avg'] = (df[\"lemmas\"] + df[\"pseudo_lemmas\"]) / 2\n",
    "df['Difference in frequency (%)'] = (df[\"lemmas\"] - df[\"pseudo_lemmas\"]) * -100 / df[\"lemmas\"]\n",
    "df = df.sort_values('avg', ascending=False).drop('avg', axis=1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Character', y='Difference in frequency (%)', hue='Difference in frequency (%)', data=df)\n",
    "plt.title('Character Distribution Comparison')\n",
    "plt.show()\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78df09b-ad0f-47ae-a942-e9569ab41475",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.viridis\n",
    "sns.set_theme(context='notebook', style=\"whitegrid\", palette=\"pastel\")\n",
    "\n",
    "counter_lem = Counter(\"\".join([i[0] for i in lemmas]))\n",
    "counter_psd = Counter(\"\".join([i[0] for i in pseudo_lemmas]))\n",
    "chars = set(\"\".join([i[0] for i in lemmas] + [i[0] for i in pseudo_lemmas]))\n",
    "print(\"First characters are the same:\", counter_psd.keys() == counter_lem.keys(), f\"{len(chars)}\")\n",
    "a = counter_psd.keys()\n",
    "print(a)\n",
    "b = list(counter_lem.keys())\n",
    "print(b)\n",
    "\n",
    "data = []\n",
    "for char in chars:\n",
    "        freq_lemmas = (counter_lem.get(char, 0) / len(lemmas_str)) * 100 if lemmas_str else 0\n",
    "        freq_pseudo_lemmas = (counter_psd.get(char, 0) / len(pseudo_lemmas_str)) * 100 if pseudo_lemmas_str else 0\n",
    "        data.append({'Character': char, \"lemmas\": freq_lemmas, \"pseudo_lemmas\": freq_pseudo_lemmas})\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['avg'] = (df[\"lemmas\"] + df[\"pseudo_lemmas\"]) / 2\n",
    "df['Difference in frequency (%)'] = (df[\"lemmas\"] - df[\"pseudo_lemmas\"]) * -100 / df[\"lemmas\"]\n",
    "df = df.sort_values('avg', ascending=False).drop('avg', axis=1)\n",
    "average_difference = df[\"Difference in frequency (%)\"].sum() / len(chars)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Character', y='Difference in frequency (%)', hue='Difference in frequency (%)', data=df)\n",
    "plt.title('First Character Distribution Comparison')\n",
    "plt.show()\n",
    "print(f\"Average difference: {average_difference:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a2476-3dd3-41c6-9388-2b28d0b147d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = {\n",
    "    \"version\": 2,\n",
    "    \"keys\": [{\"w\": l, \"r\": 200} for l in lemmas],\n",
    "    \"distractors\": [{\"w\": l, \"r\": -200} for l in pseudo_lemmas],\n",
    "}\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = f\"items.json\"\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(file, outfile, ensure_ascii=False, indent=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
