{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a8526ac-87ef-4fcb-8dc1-2abe1c105e35",
   "metadata": {},
   "source": [
    "# 2 Training the model\n",
    "Before starting, we will load a list of lemmas from the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c10d6d26-0411-431c-94a7-fac7d7541ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45271 items loaded from lemmae.json\n"
     ]
    }
   ],
   "source": [
    "# Dump the lemmas to a json file\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Change this variable to load another list of lemmas\n",
    "locale = \"fr\"\n",
    "\n",
    "# Define the file path\n",
    "file_path = f\"lemmae.json\"\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "try:\n",
    "    with open(file_path) as f:\n",
    "        content = f.read()\n",
    "        if not content.strip():\n",
    "            raise ValueError(\"The JSON file is empty.\")\n",
    "        lemmas = json.loads(content)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    lemmas = []\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    lemmas = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Invalid JSON content in {file_path}\")\n",
    "    lemmas = []\n",
    "\n",
    "print(f\"{len(lemmas)} items loaded from {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef62dc51-0327-46a9-b1af-7b7874107839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in /Users/alan/miniconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /Users/alan/miniconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (2025.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ensure you have the necessary library\n",
    "%pip install 'numpy<2', torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60daaaa-2f7a-4852-a43f-cd8d684a596f",
   "metadata": {},
   "source": [
    "## 2 Defining the Model\n",
    "\n",
    "In this part we design our network. We first initialize a PyTorch [module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) by defining the different parts of the network: an embedding layer to turn each character in a 16 dimensional vector (an array of 16 numbers), one LSTM cell (`layers_number`) that will do the actual pattern recognition and prediction work and the linear fully connected (self.fc) layer converts these predictions in a simple discrete value, i.e. the index of the next character.\n",
    "\n",
    "The forward function defines the order in which the input data will go through the network. It outputs the prediction and the updated hidden layer of the LSTM cells (these hidden states are updated even during the forward pass). And finally we have a function initializing the these hidden states with empty tensors of the good shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4df2012b-f115-405d-b12d-1553274aad5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders ready:\n",
      " <torch.utils.data.dataloader.DataLoader object at 0x11735aba0> \n",
      " <torch.utils.data.dataloader.DataLoader object at 0x10bfc8fb0>\n",
      "Model ready! Total number of parameters: 77095\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, vocab, separator_tag=None):\n",
    "        self.sequences = sequences\n",
    "        self.vocab = vocab\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "        self.idx_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
    "        if separator_tag != None:\n",
    "            self.sep_tag = separator_tag\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_seq = [self.char_to_idx[char] for char in sequence[:-1]]\n",
    "        target_seq = [self.char_to_idx[char] for char in sequence[1:]]\n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)\n",
    "\n",
    "# In this case \"vocab\" is literally the latin alphabet\n",
    "vocab = sorted(set(\"\".join(lemmas)))\n",
    "dataset = CharDataset(lemmas, vocab)\n",
    "\n",
    "random.shuffle(lemmas)\n",
    "percent_len = len(lemmas)//1000\n",
    "sequences = [\"\\n\" + \"\\n\".join(lemmas[(n-1)*percent_len:n*percent_len])+ \"\\n\" for n in range(1, 1001)]\n",
    "seq_training = sequences[:85]\n",
    "seq_validating = sequences[85:]\n",
    "vocab = sorted(set(\"\".join(sequences)))\n",
    "dataset = CharDataset(seq_training, vocab, \"\\n\")\n",
    "dataset_eval = CharDataset(seq_validating, vocab, \"\\n\")\n",
    "dataloader = DataLoader(dataset, shuffle=True)\n",
    "dataloader_eval = DataLoader(dataset_eval, shuffle=True)\n",
    "print(\"Data loaders ready:\\n\", dataloader, \"\\n\", dataloader_eval)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=4, hidden_dim=16, layers_number=1, char_to_idx={}, idx_to_char={}):\n",
    "        super().__init__()\n",
    "        vocab_size = len(char_to_idx.keys())\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.idx_to_char = idx_to_char\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, layers_number, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    # The forward function is the one getting called everytime\n",
    "    # the model created by an instance of this class is called\n",
    "    # model(x, hidden) == model.forward(x, hidden)\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return (torch.zeros(layers_number, batch_size , hidden_dim),\n",
    "                torch.zeros(layers_number, batch_size , hidden_dim))\n",
    "\n",
    "# Example usage\n",
    "embedding_dim = 8\n",
    "hidden_dim = 128\n",
    "layers_number = 1\n",
    "char_to_idx = dataset.char_to_idx\n",
    "idx_to_char = dataset.idx_to_char\n",
    "\n",
    "model = LSTMModel(embedding_dim, hidden_dim, layers_number, char_to_idx, idx_to_char)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model ready! Total number of parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46a8294-ff4f-4e51-9993-f7f793aab47b",
   "metadata": {},
   "source": [
    "# 2.3 Training\n",
    "After defining a couple of hyperparameters, we are ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bab21c79-973d-477e-884a-de26e794b35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██▊                         | 1/10 [00:06<01:00,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 1.7494, Validation Loss: 1.7321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████▌                      | 2/10 [00:13<00:54,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Loss: 1.7269, Validation Loss: 1.7748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████▍                   | 3/10 [00:20<00:46,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Training Loss: 1.7277, Validation Loss: 1.6509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████▏                | 4/10 [00:26<00:40,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Training Loss: 1.7165, Validation Loss: 1.6619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████              | 5/10 [00:33<00:33,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Training Loss: 1.7247, Validation Loss: 1.7111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████▊           | 6/10 [00:40<00:26,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Training Loss: 1.7102, Validation Loss: 1.6567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████▌        | 7/10 [00:47<00:20,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Training Loss: 1.7029, Validation Loss: 1.7284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████▍     | 8/10 [00:53<00:13,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Training Loss: 1.7033, Validation Loss: 1.7126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████▏  | 9/10 [01:00<00:06,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Training Loss: 1.6912, Validation Loss: 1.7395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10/10 [01:07<00:00,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Training Loss: 1.6855, Validation Loss: 1.7153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.005\n",
    "vocab_size = len(char_to_idx)\n",
    "\n",
    "# Loss function and optimizer\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    \n",
    "    random.shuffle(lemmas)\n",
    "    percent_len = len(lemmas)//1000\n",
    "    sequences = [\"\\n\" + \"\\n\".join(lemmas[(n-1)*percent_len:n*percent_len])+ \"\\n\" for n in range(1, 1001)]\n",
    "    seq_training = sequences[:900]\n",
    "    seq_validating = sequences[900:]\n",
    "    vocab = sorted(set(\"\".join(sequences)))\n",
    "    dataset = CharDataset(seq_training, vocab, \"\\n\")\n",
    "    dataset_eval = CharDataset(seq_validating, vocab, \"\\n\")\n",
    "    dataloader = DataLoader(dataset, shuffle=True)\n",
    "    dataloader_eval = DataLoader(dataset_eval, shuffle=True)\n",
    "    \n",
    "    # first, train the model\n",
    "    model.train()\n",
    "    hidden = model.init_hidden()\n",
    "    training_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, hidden = model.forward(inputs, hidden)\n",
    "        loss = cross_entropy(outputs.view(-1, vocab_size), targets.squeeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss = loss.item()\n",
    "        hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "        \n",
    "    # second, evaluate the model to avoid overfitting\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader_eval:\n",
    "        hidden = model.init_hidden()\n",
    "\n",
    "        # forward pass\n",
    "        outputs, hidden = model.forward(inputs, hidden)\n",
    "        loss = cross_entropy(outputs.view(-1, vocab_size), targets.squeeze(0))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss_eval = total_loss / len(dataloader_eval)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss_eval:.4f}, Validation Loss: {training_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780ceed-d578-482e-ae4c-955fb7cd7794",
   "metadata": {},
   "source": [
    "## 2.4 Generating the Pseudo-words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0f6dbf8-1109-4f2d-b723-296bfb576c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 words so far\n",
      "abas-fourre\n",
      "abjincure-dame-thèbe\n",
      "abprinateur\n",
      "abrop\n",
      "abroyal\n",
      "accolurger\n",
      "accordure\n",
      "accorfette\n",
      "accoutement\n",
      "accusiblement\n",
      "véroprant\n",
      "wyomatique\n",
      "zombisme\n",
      "zoolan\n",
      "zubaison\n",
      "à carre\n",
      "ébouteur\n",
      "échandeuse\n",
      "échanguare\n",
      "échant-vouffe\n",
      "échariner\n",
      "échau\n",
      "échausselle\n",
      "échoser\n",
      "éculaire\n",
      "écumisme\n",
      "éfouillage\n",
      "élagat\n",
      "élagrament\n",
      "électraler\n",
      "électrication\n",
      "électropher\n",
      "émissible\n",
      "épagnation\n",
      "épharal\n",
      "épielle\n",
      "épiphtath\n",
      "épispiculon\n",
      "épissers\n",
      "épreisse\n",
      "éprissant\n",
      "étingue\n",
      "étragme\n",
      "étrenamiser\n",
      "éténisme\n",
      "évalence\n",
      "évaniminer\n",
      "évantage\n",
      "évaufonctorisé\n",
      "œillon-gradio\n",
      "10.38\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "import torch.nn.functional as F\n",
    "from spylls.hunspell import Dictionary\n",
    "import sys\n",
    "dictionary = Dictionary.from_files(f\"hunspell/{locale}\")\n",
    "\n",
    "\n",
    "def generate_pseudoword(model, length=15, temperature=0.87):\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(1)\n",
    "    start_seq = [0]\n",
    "    inputs = torch.tensor(start_seq).unsqueeze(0)  # Shape: (1, seq_len)\n",
    "    generated_seq = []\n",
    "    words_generated = set([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while len(words_generated) < length:\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "\n",
    "            # outputs shape: (1, seq_len, vocab_size)\n",
    "            # We need the last time step's output for the next prediction\n",
    "            last_output = outputs[:, -1]  # Shape: (1, vocab_size)\n",
    "\n",
    "            # Apply temperature scaling\n",
    "            last_output = last_output / temperature\n",
    "            probs = F.softmax(last_output, dim=-1).squeeze(0)  # the multinomial accepts only one order tensors\n",
    "\n",
    "            # Ensure all the probabilities are valid\n",
    "            if torch.isnan(probs).any() or torch.isinf(probs).any() or (probs < 0).any():\n",
    "                print(\"Invalid probabilities detected. Resetting to uniform distribution.\")\n",
    "                probs = torch.ones_like(probs) / probs.size(0)\n",
    "\n",
    "            # Sample the next character\n",
    "            predicted_idx = torch.multinomial(probs, 1).item()\n",
    "            generated_seq.append(predicted_idx)\n",
    "            inputs = torch.tensor([[predicted_idx]])  # Shape: (1, 1)\n",
    "\n",
    "            if vocab[predicted_idx] == \"\\n\":\n",
    "                new_word = ''.join([vocab[i] for i in generated_seq[:-1]])\n",
    "                generated_seq = []\n",
    "                if not dictionary.lookup(new_word.capitalize()) and new_word not in lemmas:\n",
    "                    words_generated.add(new_word)\n",
    "                sys.stdout.write(f\"\\r{len(words_generated)} words so far\")\n",
    "\n",
    "    return list(sorted(words_generated))\n",
    "\n",
    "# Example usage\n",
    "generated_pseudoword = generate_pseudoword(model, 1000)\n",
    "print()\n",
    "print(\"\\n\".join(generated_pseudoword[:10]))\n",
    "print(\"\\n\".join(generated_pseudoword[-40:]))\n",
    "print(len(\"\\n\".join(generated_pseudoword))/len(generated_pseudoword))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abcb387-06cb-4ba1-9814-f430f0e8680e",
   "metadata": {},
   "source": [
    "# 5 Saving and loading our results\n",
    "\n",
    "If you are happy with the results, like the loss, especially against the validation set, and the words generated, you can run the following block to save the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a746a724-ded3-4540-b1e1-7d4cebb9236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model you've trained so far\n",
    "torch.save(model, f'lstm_model-{locale}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956279a-cb38-464b-a083-44a0db6f028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate words from the the last version of the model you saved\n",
    "model = torch.load(f'locales/{locale}/lstm_model-{locale}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f09fe4-9405-4e01-bb88-05828bfba08e",
   "metadata": {},
   "source": [
    "We can now generate our pseudo-lexicon. To find it, look out for the pseudo-lemmas.json file in the dictionary folder of your source dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1460bcd8-825a-4b04-98bf-0f1890c8fa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 words so far\n",
      "50000 pseudo words successfully generated and loaded in 9:27.460\n"
     ]
    }
   ],
   "source": [
    "# Dump the lemmas to a json file\n",
    "import json\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = f\"pseudo-lemmae.json\"\n",
    "\n",
    "generated_pseudoword = generate_pseudoword(model, 50000)\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(generated_pseudoword, outfile, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "time = time.time() - start_time\n",
    "\n",
    "print(f\"{len(generated_pseudoword)} pseudo words successfully generated and loaded in {time//60:.0f}:{(time%60):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d09109-7ad7-4bbf-853e-ac72d343f0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
