{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd4bd97b-e7d9-4063-9130-8d4a3f0088f3",
   "metadata": {},
   "source": [
    "# 2 Training the model\n",
    "Before starting, we will load a list of lemmas from the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1d93ce1f-55f6-49e1-b0a0-f9d4d0ffe5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30614 items loaded from locales/en-GB/lemmas.json\n"
     ]
    }
   ],
   "source": [
    "# Dump the lemmas to a json file\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Change this variable to load another list of lemmas\n",
    "locale = \"en-GB\"\n",
    "\n",
    "# Define the file path\n",
    "file_path = f\"locales/{locale}/lemmas.json\"\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "try:\n",
    "    with open(file_path) as f:\n",
    "        content = f.read()\n",
    "        if not content.strip():\n",
    "            raise ValueError(\"The JSON file is empty.\")\n",
    "        lemmas = json.loads(content)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    lemmas = []\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    lemmas = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Invalid JSON content in {file_path}\")\n",
    "    lemmas = []\n",
    "\n",
    "print(f\"{len(lemmas)} items loaded from {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e85c7",
   "metadata": {},
   "source": [
    "## 2.1 Data Preparation\n",
    "Now we can start tokenizing our data. In the context of a character-level language model, tokenizing means to turn the words that us human can read into sequences of numbers that the model can interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "841ce465-aad1-4b8b-b1c7-30bbbe94f8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in /Users/alan/miniconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /Users/alan/miniconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/alan/miniconda3/lib/python3.12/site-packages (from torch) (2025.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/alan/miniconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ensure you have the necessary library\n",
    "%pip install 'numpy<2', torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "722c32a6-5932-4547-93c7-56ac01515ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, vocab, separator_tag=None):\n",
    "        self.sequences = sequences\n",
    "        self.vocab = vocab\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "        self.idx_to_char = {idx: char for idx, char in enumerate(vocab)}\n",
    "        if separator_tag != None:\n",
    "            self.sep_tag = separator_tag\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_seq = [self.char_to_idx[char] for char in sequence[:-1]]\n",
    "        target_seq = [self.char_to_idx[char] for char in sequence[1:]]\n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)\n",
    "\n",
    "# In this case \"vocab\" is literally the latin alphabet\n",
    "vocab = sorted(set(\"\".join(lemmas)))\n",
    "dataset = CharDataset(lemmas, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c0c350-259c-413f-8313-6bed54cafbac",
   "metadata": {},
   "source": [
    "This loaded the lemmas in a dataset in a format that torch can understand. Each word is turned in a pair of sequences, an input (missing the last character) and a target (missing the first character). In this case, because the input sequences start with an added \"start of sequence\" special token, the target sequence is the full word. In plain English, this means that we also want our model to learn what is the most likely first letter of a word, not only the next most likely character based on the beginning of the sequence. \n",
    "\n",
    "All the characters are converted to numbers, each being the index of the input neuron that will be activated during the training. The system has as many inputs neurons, or input dimensions, as there are items in the vocabulary (by vocabulary, we mean alphabet). This is a reasonable number that allows the model to train on any computer, but imagine the size of a model when the vocabulary contains hundred of thousands of words (from different languages), and that each one needs its own input neuron... \n",
    "\n",
    "Run the following block to see how your data will be processed by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "baf79685-949a-4e88-8d1b-7be77290ed95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== steamy == \n",
      "becomes the sequences:\n",
      "tensor([18, 19,  4,  0, 12]) (input)\n",
      "and tensor([19,  4,  0, 12, 24]) (target)\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "n = randrange(len(lemmas))\n",
    "\n",
    "print(f\"== {lemmas[n]} == \\nbecomes the sequences:\\n{dataset[n][0]} (input)\\nand {dataset[n][1]} (target)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f35c012-ccf3-47dc-9bb7-2724b765747d",
   "metadata": {},
   "source": [
    "### 1.2 Grouping the sequences to learn\n",
    "For convenience during both training and generation, we'll group the words in lists of a percent of the total number of words and separate each word by a special newline character \"\\n\". We also extract five sequences for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "14dcabc6-40f6-4ed5-abdc-36b3c041c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(lemmas)\n",
    "percent_len = len(lemmas)//100\n",
    "sequences = [\"\\n\" + \"\\n\".join(lemmas[(n-1)*percent_len:n*percent_len])+ \"\\n\" for n in range(1, 101)]\n",
    "seq_training = sequences[:95]\n",
    "seq_validating = sequences[95:]\n",
    "vocab = sorted(set(\"\".join(sequences)))\n",
    "dataset = CharDataset(seq_training, vocab, \"\\n\")\n",
    "dataset_eval = CharDataset(seq_validating, vocab, \"\\n\")\n",
    "dataloader = DataLoader(dataset, shuffle=True)\n",
    "dataloader_eval = DataLoader(dataset_eval, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247b706-7e46-4bed-962d-7f9ef9cbe1e9",
   "metadata": {},
   "source": [
    "## 2.2 Defining the Model\n",
    "\n",
    "In this part we design our network. We first initialize a PyTorch [module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) by defining the different parts of the network: an embedding layer to turn each character in a 16 dimensional vector (an array of 16 numbers), one LSTM cell (`layers_number`) that will do the actual pattern recognition and prediction work and the linear fully connected (self.fc) layer converts these predictions in a simple discrete value, i.e. the index of the next character.\n",
    "\n",
    "The forward function defines the order in which the input data will go through the network. It outputs the prediction and the updated hidden layer of the LSTM cells (these hidden states are updated even during the forward pass). And finally we have a function initializing the these hidden states with empty tensors of the good shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0631f9e1-1050-4c1c-914d-066aec50550e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready! Total number of parameters: 1975\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, layers_number, char_to_idx, idx_to_char):\n",
    "        super().__init__()\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.idx_to_char = idx_to_char\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, layers_number, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    # The forward function is the one getting called everytime\n",
    "    # the model created by an instance of this class is called\n",
    "    # model(x, hidden) == model.forward(x, hidden)\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return (torch.zeros(layers_number, batch_size , hidden_dim),\n",
    "                torch.zeros(layers_number, batch_size , hidden_dim))\n",
    "\n",
    "# Example usage\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 4\n",
    "hidden_dim = 16\n",
    "layers_number = 1\n",
    "char_to_idx = dataset.char_to_idx\n",
    "idx_to_char = dataset.idx_to_char\n",
    "\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, layers_number, char_to_idx, idx_to_char)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Model ready! Total number of parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e3fadd-7c89-4f46-8727-f58a5f735522",
   "metadata": {},
   "source": [
    "# 2.3 Training\n",
    "After defining a couple of hyperparameters, we are ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fe522c6a-bc95-49b8-b92c-a46d83f1b0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████▌                                                 | 1/10 [00:01<00:11,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 2.2231, Validation Loss: 2.2011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████                                            | 2/10 [00:02<00:10,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Loss: 2.2221, Validation Loss: 2.2357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████▌                                      | 3/10 [00:03<00:08,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Training Loss: 2.2260, Validation Loss: 2.2207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████                                 | 4/10 [00:05<00:07,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Training Loss: 2.2351, Validation Loss: 2.2387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████▌                           | 5/10 [00:06<00:06,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Training Loss: 2.2287, Validation Loss: 2.2436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████                      | 6/10 [00:07<00:04,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Training Loss: 2.2245, Validation Loss: 2.2255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████▌                | 7/10 [00:08<00:03,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Training Loss: 2.2311, Validation Loss: 2.2211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████           | 8/10 [00:10<00:02,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Training Loss: 2.2329, Validation Loss: 2.2357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████▌     | 9/10 [00:11<00:01,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Training Loss: 2.2232, Validation Loss: 2.2250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Training Loss: 2.2190, Validation Loss: 2.1690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Loss function and optimizer\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # first, train the model\n",
    "    model.train()\n",
    "    hidden = model.init_hidden()\n",
    "    training_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, hidden = model.forward(inputs, hidden)\n",
    "        loss = cross_entropy(outputs.view(-1, vocab_size), targets.squeeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss = loss.item()\n",
    "        hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "        \n",
    "    # second, evaluate the model to avoid overfitting\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader_eval:\n",
    "        hidden = model.init_hidden()\n",
    "\n",
    "        # forward pass\n",
    "        outputs, hidden = model.forward(inputs, hidden)\n",
    "        loss = cross_entropy(outputs.view(-1, vocab_size), targets.squeeze(0))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss_eval = total_loss / len(dataloader_eval)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss_eval:.4f}, Validation Loss: {training_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7540a45-2ee6-4d18-9548-7af6b5082825",
   "metadata": {},
   "source": [
    "### 2.4 Sampling generated sequences\n",
    "\n",
    "In the following block, we can see how the model generates an array of probability for each character of the input sequence after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "234c21cb-68ea-4c5d-a4d0-b190a1640c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Next character weights vector (values below zero are set to zero):\n",
      " tensor([0.0000, 0.0000, 0.4551, 0.7840, 0.6686, 0.0000, 0.1316, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.8063, 0.4040, 1.0445, 0.0000, 0.4470, 0.0000,\n",
      "        1.0074, 0.8156, 1.0094, 0.0000, 0.3994, 0.0000, 0.4233, 0.0000, 0.0000])\n",
      "\n",
      "Weights for the next charater with temperature scaling:\n",
      " tensor([[  0.0000,   0.0000,  45.5119,  78.4001,  66.8647,   0.0000,  13.1559,\n",
      "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  80.6310,  40.3999,\n",
      "         104.4481,   0.0000,  44.6953,   0.0000, 100.7406,  81.5613, 100.9435,\n",
      "           0.0000,  39.9361,   0.0000,  42.3345,   0.0000,   0.0000]])\n",
      "\n",
      "Probabilities for the next charater after scaling and with the softmax function:\n",
      " tensor([0.0000e+00, 0.0000e+00, 2.4056e-26, 4.6173e-12, 4.5148e-17, 0.0000e+00,\n",
      "        2.1341e-40, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        4.2979e-11, 1.4491e-28, 9.4823e-01, 0.0000e+00, 1.0631e-26, 0.0000e+00,\n",
      "        2.3268e-02, 1.0896e-10, 2.8503e-02, 0.0000e+00, 9.1142e-29, 0.0000e+00,\n",
      "        1.0031e-27, 0.0000e+00, 0.0000e+00])\n",
      "Previous characters: ['\\n', 'c', 'a']\n",
      "Generated character: n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x128a158e0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# First we disable the gradient calculation because we won't need it (no more backpropagation after the training)\n",
    "# This makes the tensor representation cleaner\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "hidden = model.init_hidden(1)\n",
    "start_seq = [0, 3, 1]\n",
    "inputs = torch.tensor(start_seq).unsqueeze(0)  # Shape: (1, seq_len)\n",
    "\n",
    "outputs, hidden = model(inputs, hidden) # short for model.forward(inputs, hidden)\n",
    "\n",
    "last_output = outputs[:, -1]\n",
    "last_output[torch.where(last_output<0)] = 0\n",
    "print(\"\\nNext character weights vector (values below zero are set to zero):\\n\", last_output[0])\n",
    "\n",
    "\n",
    "temperature = 0.01\n",
    "last_output = last_output / temperature\n",
    "\n",
    "print(\"\\nWeights for the next charater with temperature scaling:\\n\", last_output)\n",
    "\n",
    "probabilities = F.softmax(last_output, dim=-1).squeeze(0)\n",
    "\n",
    "print(\"\\nProbabilities for the next charater after scaling and with the softmax function:\\n\", probabilities)\n",
    "\n",
    "# This is where the magic happens\n",
    "# the mutinomial method samples (in this case) one item following the weights of the probability vector it recieves\n",
    "predicted_idx = torch.multinomial(probabilities, 1).item()\n",
    "\n",
    "print(\"Previous characters:\", [dataset.idx_to_char[i] for i in start_seq])\n",
    "print(\"Generated character:\", dataset.idx_to_char[predicted_idx])\n",
    "\n",
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e932ee-c07e-4e35-a5c0-8e91339b7448",
   "metadata": {},
   "source": [
    "Now we can try to generate some pseudo-words to \"vibe check\" how good is our newly trained model at the task it was created for, instead of cold cross-entropy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "612970dc-7d14-4be0-b48e-d056a654696f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carcoopa\n",
      "cartrill\n",
      "chincon\n",
      "chocogropotate\n",
      "extray\n",
      "giboom\n",
      "malion\n",
      "resedy\n",
      "stenant\n",
      "tillie\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "import torch.nn.functional as F\n",
    "from spylls.hunspell import Dictionary\n",
    "\n",
    "dictionary = Dictionary.from_files(f\"locales/{locale}/{locale}\")\n",
    "\n",
    "\n",
    "def generate_pseudoword(model, length=15, temperature=0.6):\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(1)\n",
    "    start_seq = [0]\n",
    "    inputs = torch.tensor(start_seq).unsqueeze(0)  # Shape: (1, seq_len)\n",
    "    generated_seq = []\n",
    "    words_generated = set([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while len(words_generated) < length:\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "\n",
    "            # outputs shape: (1, seq_len, vocab_size)\n",
    "            # We need the last time step's output for the next prediction\n",
    "            last_output = outputs[:, -1]  # Shape: (1, vocab_size)\n",
    "\n",
    "            # Apply temperature scaling\n",
    "            last_output = last_output / temperature\n",
    "            probs = F.softmax(last_output, dim=-1).squeeze(0)  # the multinomial accepts only one order tensors\n",
    "\n",
    "            # Ensure all the probabilities are valid\n",
    "            if torch.isnan(probs).any() or torch.isinf(probs).any() or (probs < 0).any():\n",
    "                print(\"Invalid probabilities detected. Resetting to uniform distribution.\")\n",
    "                probs = torch.ones_like(probs) / probs.size(0)\n",
    "\n",
    "            # Sample the next character\n",
    "            predicted_idx = torch.multinomial(probs, 1).item()\n",
    "            generated_seq.append(predicted_idx)\n",
    "            inputs = torch.tensor([[predicted_idx]])  # Shape: (1, 1)\n",
    "\n",
    "            if vocab[predicted_idx] == \"\\n\":\n",
    "                new_word = ''.join([vocab[i] for i in generated_seq[:-1]])\n",
    "                generated_seq = []\n",
    "                if not dictionary.lookup(new_word):\n",
    "                    words_generated.add(new_word)\n",
    "\n",
    "    return list(sorted(words_generated))\n",
    "\n",
    "# Example usage\n",
    "generated_pseudoword = generate_pseudoword(model, 10)\n",
    "print(\"\\n\".join(generated_pseudoword))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978525b-e172-458c-8118-7a76ac648140",
   "metadata": {},
   "source": [
    "# 5 Saving and dumping our results\n",
    "\n",
    "If you are happy with the results, like the loss, especially against the validation set, and the words generated, you can run the following block to save the model's weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bd305b3d-a416-4cd1-ad79-7ba2261bc5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model you've trained so far\n",
    "torch.save(model, f'locales/{locale}/lstm_model-{locale}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed2ef49-9257-4b1e-84ab-36ba82ade5af",
   "metadata": {},
   "source": [
    "Or use this block to load a previously saved model to generate more non-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a5621bda-d3a6-4d1c-be09-3c278678dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate words from the the last version of the model you saved\n",
    "model = torch.load(f'locales/{locale}/lstm_model-{locale}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945c63e-106f-4796-8493-2bba6d896a8b",
   "metadata": {},
   "source": [
    "We can now generate our pseudo-lexicon. To find it, look out for the pseudo-lemmas.json file in the dictionary folder of your source dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c0aff386-e847-41b1-9b89-76b6bf90df44",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dictionaries/en-GB/pseudo-lemmas.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m generated_pseudoword = generate_pseudoword(model, \u001b[32m10000\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Write the lemmas list to the JSON file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[32m     13\u001b[39m     json.dump(generated_pseudoword, outfile, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m, indent=\u001b[32m4\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(generated_pseudoword)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pseudo words successfully generated and loaded in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'dictionaries/en-GB/pseudo-lemmas.json'"
     ]
    }
   ],
   "source": [
    "# Dump the lemmas to a json file\n",
    "import json\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = f\"dictionaries/{locale}/pseudo-lemmas.json\"\n",
    "\n",
    "generated_pseudoword = generate_pseudoword(model, 10000)\n",
    "\n",
    "# Write the lemmas list to the JSON file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(generated_pseudoword, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "    \n",
    "print(f\"{len(generated_pseudoword)} pseudo words successfully generated and loaded in {(time.time() - start_time):.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
